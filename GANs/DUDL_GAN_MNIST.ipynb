{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab74482",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ydebessu/DeepUnderstandingOfDeepLearning/blob/main/GANs/DUDL_GAN_MNIST.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Generative adversarial networks\n",
    "### LECTURE: Linear GAN with MNIST\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpcmh-V8hIlw"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpUeQWVfBJbY"
   },
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfZKI3EXBHL5"
   },
   "outputs": [],
   "source": [
    "# import dataset (comes with colab!)\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# don't need the labels here\n",
    "data = data[:,1:]\n",
    "\n",
    "# normalize the data to a range of [-1 1] (b/c tanh output)\n",
    "dataNorm = data / np.max(data)\n",
    "dataNorm = 2*dataNorm - 1\n",
    "\n",
    "# convert to tensor\n",
    "dataT = torch.tensor( dataNorm ).float()\n",
    "\n",
    "# no dataloaders!\n",
    "batchsize = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vvglaJyCMpO"
   },
   "source": [
    "# Create classes for the discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UT-TyZZDK9-9"
   },
   "outputs": [],
   "source": [
    "class discriminatorNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.fc1 = nn.Linear(28*28,256)\n",
    "    self.fc2 = nn.Linear(256,256)\n",
    "    self.out = nn.Linear(256,1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = F.leaky_relu( self.fc1(x) )\n",
    "    x = F.leaky_relu( self.fc2(x) )\n",
    "    x = self.out(x)\n",
    "    return torch.sigmoid( x )\n",
    "\n",
    "dnet = discriminatorNet()\n",
    "y = dnet(torch.randn(10,784))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alVVPOJiLTHB"
   },
   "outputs": [],
   "source": [
    "class generatorNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.fc1 = nn.Linear(64,256)\n",
    "    self.fc2 = nn.Linear(256,256)\n",
    "    self.out = nn.Linear(256,784)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = F.leaky_relu( self.fc1(x) )\n",
    "    x = F.leaky_relu( self.fc2(x) )\n",
    "    x = self.out(x)\n",
    "    return torch.tanh( x )\n",
    "\n",
    "\n",
    "gnet = generatorNet()\n",
    "y = gnet(torch.randn(10,64))\n",
    "plt.imshow(y[0,:].detach().squeeze().view(28,28));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBsOOqcX_LvO"
   },
   "source": [
    "# Train the models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFDbnRqeCPqy"
   },
   "outputs": [],
   "source": [
    "# loss function (same for both phases of training)\n",
    "lossfun = nn.BCELoss()\n",
    "\n",
    "# create instances of the models\n",
    "dnet = discriminatorNet().to(device)\n",
    "gnet = generatorNet().to(device)\n",
    "\n",
    "# optimizers (same algo but different variables b/c different parameters)\n",
    "d_optimizer = torch.optim.Adam(dnet.parameters(), lr=.0003)\n",
    "g_optimizer = torch.optim.Adam(gnet.parameters(), lr=.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83Ju8fDuUTBg"
   },
   "outputs": [],
   "source": [
    "# this cell takes ~3 mins with 50k epochs\n",
    "num_epochs = 50000\n",
    "\n",
    "losses  = np.zeros((num_epochs,2))\n",
    "disDecs = np.zeros((num_epochs,2)) # disDecs = discriminator decisions\n",
    "\n",
    "for epochi in range(num_epochs):\n",
    "    \n",
    "  # create minibatches of REAL and FAKE images \n",
    "  randidx     = torch.randint(dataT.shape[0],(batchsize,))\n",
    "  real_images = dataT[randidx,:].to(device)\n",
    "  fake_images = gnet( torch.randn(batchsize,64).to(device) ) # output of generator\n",
    "\n",
    "\n",
    "  # labels used for real and fake images\n",
    "  real_labels = torch.ones(batchsize,1).to(device)\n",
    "  fake_labels = torch.zeros(batchsize,1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "  ### ---------------- Train the discriminator ---------------- ###\n",
    "\n",
    "  # forward pass and loss for REAL pictures\n",
    "  pred_real   = dnet(real_images)              # REAL images into discriminator\n",
    "  d_loss_real = lossfun(pred_real,real_labels) # all labels are 1\n",
    "  \n",
    "  # forward pass and loss for FAKE pictures\n",
    "  pred_fake   = dnet(fake_images)              # FAKE images into discriminator\n",
    "  d_loss_fake = lossfun(pred_fake,fake_labels) # all labels are 0\n",
    "  \n",
    "  # collect loss (using combined losses)\n",
    "  d_loss = d_loss_real + d_loss_fake\n",
    "  losses[epochi,0]  = d_loss.item()\n",
    "  disDecs[epochi,0] = torch.mean((pred_real>.5).float()).detach()\n",
    "\n",
    "  # backprop\n",
    "  d_optimizer.zero_grad()\n",
    "  d_loss.backward()\n",
    "  d_optimizer.step()\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "  ### ---------------- Train the generator ---------------- ###\n",
    "\n",
    "  # create fake images and compute loss\n",
    "  fake_images = gnet( torch.randn(batchsize,64).to(device) )\n",
    "  pred_fake   = dnet(fake_images)\n",
    "  \n",
    "  # compute and collect loss and accuracy\n",
    "  g_loss = lossfun(pred_fake,real_labels)\n",
    "  losses[epochi,1]  = g_loss.item()\n",
    "  disDecs[epochi,1] = torch.mean((pred_fake>.5).float()).detach()\n",
    "  \n",
    "  # backprop\n",
    "  g_optimizer.zero_grad()\n",
    "  g_loss.backward()\n",
    "  g_optimizer.step()\n",
    "\n",
    "  \n",
    "  # print out a status message\n",
    "  if (epochi+1)%500==0:\n",
    "    msg = f'Finished epoch {epochi+1}/{num_epochs}'\n",
    "    sys.stdout.write('\\r' + msg)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1C0qAf9kN7mi"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(18,5))\n",
    "\n",
    "ax[0].plot(losses)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Model loss')\n",
    "ax[0].legend(['Discrimator','Generator'])\n",
    "# ax[0].set_xlim([4000,5000])\n",
    "\n",
    "ax[1].plot(losses[::5,0],losses[::5,1],'k.',alpha=.1)\n",
    "ax[1].set_xlabel('Discriminator loss')\n",
    "ax[1].set_ylabel('Generator loss')\n",
    "\n",
    "ax[2].plot(disDecs)\n",
    "ax[2].set_xlabel('Epochs')\n",
    "ax[2].set_ylabel('Probablity (\"real\")')\n",
    "ax[2].set_title('Discriminator output')\n",
    "ax[2].legend(['Real','Fake'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElnXz0ZkS8Yc"
   },
   "source": [
    "# Let's see some fake digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzCz1UqGCP8T"
   },
   "outputs": [],
   "source": [
    "# generate the images from the generator network\n",
    "gnet.eval()\n",
    "fake_data = gnet(torch.randn(12,64).to(device)).cpu()\n",
    "\n",
    "# and visualize...\n",
    "fig,axs = plt.subplots(3,4,figsize=(8,6))\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "  ax.imshow(fake_data[i,:,].detach().view(28,28),cmap='gray')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pU-I5tvmCP-6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Q1Cx6X9i0H-"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfVbtROJi0K2"
   },
   "outputs": [],
   "source": [
    "# 1) I tried adding batch normalization to the models, but the results weren't that nice. Can you guess why? Try adding\n",
    "#    batchnorm after each layer (except the output) and observe the effects. Can you explain why the results are the\n",
    "#    way they are? (Note: batchnorm becomes important in deeper CNN GANs.)\n",
    "# \n",
    "# 2) Re-running the same code to show the fake images returns different digits each time. Fix PyTorch's random seed so \n",
    "#    that the random numbers are identical each time you run the code. Are the images still different on multiple runs?\n",
    "# \n",
    "# 3) To see how the generator is progressing, you can create some images during training. Here's what to do: (1) put the\n",
    "#    image-generation code above inside a function. (2) Modify that function so that the figure is saved to a file.\n",
    "#    (3) Modify the training function so that it calls the plotting function every 5000 epochs (or whatever resolution\n",
    "#    you want). Then you can see how the images look more like digits as the generator model learns!\n",
    "# \n",
    "# 4) GANs can be quite sensitive to the learning rate, because you are training two different but interacting networks\n",
    "#    at the same time. Usually a good strategy is to have a very small learning rate and train for a long time. But don't\n",
    "#    take my advice -- try a much larger learning rate for a shorter period of time, and see what happens!\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPlzEW5LV2osq7FEpHisaLq",
   "collapsed_sections": [],
   "name": "DUDL_GAN_MNIST.ipynb",
   "provenance": [
    {
     "file_id": "1W9fGz1EYzDhtHHpBYU6M2fEpi9Q1uXez",
     "timestamp": 1620754493662
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
