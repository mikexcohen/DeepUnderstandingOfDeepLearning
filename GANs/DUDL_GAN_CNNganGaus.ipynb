{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DUDL_GAN_CNNganGaus.ipynb",
   "provenance": [
    {
     "file_id": "1MaSmZUtUwcHHxPQCw6c1eHrz2MrEgKUs",
     "timestamp": 1620882064830
    },
    {
     "file_id": "18Pq-U92YbGrtIVYoI_ju2ShgqyXtQPvu",
     "timestamp": 1620850688381
    },
    {
     "file_id": "1x_oQTpEyKOkoa8AzE54JcqQKcc2PxrY_",
     "timestamp": 1620837440064
    },
    {
     "file_id": "1t-RwrGO8-BWLf66uZZvWyFwAXjslhFMe",
     "timestamp": 1620794140775
    },
    {
     "file_id": "1W9fGz1EYzDhtHHpBYU6M2fEpi9Q1uXez",
     "timestamp": 1620754493662
    }
   ],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyOek5kOD93J9B736gUcIX0O"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Generative adversarial networks\n",
    "### LECTURE: CNN GAN with Gaussians\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lpcmh-V8hIlw"
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpUeQWVfBJbY"
   },
   "source": [
    "# Create the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dLedkm063gpO"
   },
   "source": [
    "nImages = 3000\n",
    "imgSize = 64\n",
    "\n",
    "x   = np.linspace(-4,4,imgSize)\n",
    "X,Y = np.meshgrid(x,x)\n",
    "\n",
    "\n",
    "# initialize tensors containing images and labels\n",
    "images = torch.zeros(nImages,1,imgSize,imgSize)\n",
    "\n",
    "for i in range(nImages):\n",
    "\n",
    "  # create the gaussian with random centers\n",
    "  ro = 2*np.random.randn(2) # ro = random offset\n",
    "  width = np.random.rand()/.6 + 1.8 # random width\n",
    "  G  = np.exp( -( (X-ro[0])**2 + (Y-ro[1])**2) / (2*width**2) )\n",
    "  \n",
    "  # and add noise\n",
    "  G  = G + np.random.randn(imgSize,imgSize)/5\n",
    "  \n",
    "  # add to the tensor\n",
    "  images[i,:,:,:] = torch.Tensor(G).view(1,imgSize,imgSize)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kkHHEUXt6J57"
   },
   "source": [
    "# visualize some images\n",
    "fig,axs = plt.subplots(3,7,figsize=(13,6))\n",
    "\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "  whichpic = np.random.randint(nImages)\n",
    "  G = np.squeeze( images[whichpic,:,:] )\n",
    "  ax.imshow(G,vmin=-1,vmax=1,cmap='jet')\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vvglaJyCMpO"
   },
   "source": [
    "# Create classes for the discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IINSmrEvN--Z"
   },
   "source": [
    "# Architecture and meta-parameter choices were inspired by https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UT-TyZZDK9-9"
   },
   "source": [
    "class discriminatorNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # convolution layers\n",
    "    self.conv1 = nn.Conv2d(  1, 64, 4, 2, 1, bias=False)\n",
    "    self.conv2 = nn.Conv2d( 64,128, 4, 2, 1, bias=False)\n",
    "    self.conv3 = nn.Conv2d(128,256, 4, 2, 1, bias=False)\n",
    "    self.conv4 = nn.Conv2d(256,512, 4, 2, 1, bias=False)\n",
    "    self.conv5 = nn.Conv2d(512,  1, 4, 1, 0, bias=False)\n",
    "\n",
    "    # batchnorm\n",
    "    self.bn2 = nn.BatchNorm2d(128)\n",
    "    self.bn3 = nn.BatchNorm2d(256)\n",
    "    self.bn4 = nn.BatchNorm2d(512)\n",
    "    \n",
    "  def forward(self,x):\n",
    "    # print(x.shape)\n",
    "    x = F.leaky_relu( self.conv1(x) ,.2)\n",
    "    # print(x.shape)\n",
    "    x = F.leaky_relu( self.conv2(x) ,.2)\n",
    "    # print(x.shape)\n",
    "    x = self.bn2(x)\n",
    "    x = F.leaky_relu( self.conv3(x) ,.2)\n",
    "    # print(x.shape)\n",
    "    x = self.bn3(x)\n",
    "    x = F.leaky_relu( self.conv4(x) ,.2)\n",
    "    # print(x.shape)\n",
    "    x = self.bn4(x)\n",
    "    return torch.sigmoid( self.conv5(x) ).view(-1,1)\n",
    "\n",
    "\n",
    "dnet = discriminatorNet()\n",
    "y = dnet(torch.randn(10,1,64,64))\n",
    "y.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "alVVPOJiLTHB"
   },
   "source": [
    "class generatorNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # convolution layers\n",
    "    self.conv1 = nn.ConvTranspose2d(100,512, 4, 1, 0, bias=False)\n",
    "    self.conv2 = nn.ConvTranspose2d(512,256, 4, 2, 1, bias=False)\n",
    "    self.conv3 = nn.ConvTranspose2d(256,128, 4, 2, 1, bias=False)\n",
    "    self.conv4 = nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False)\n",
    "    self.conv5 = nn.ConvTranspose2d(64,   1, 4, 2, 1, bias=False)\n",
    "\n",
    "    # batchnorm\n",
    "    self.bn1 = nn.BatchNorm2d(512)\n",
    "    self.bn2 = nn.BatchNorm2d(256)\n",
    "    self.bn3 = nn.BatchNorm2d(128)\n",
    "    self.bn4 = nn.BatchNorm2d( 64)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    print(x.shape)\n",
    "    x = F.relu( self.bn1(self.conv1(x)) )\n",
    "    print(x.shape)\n",
    "    x = F.relu( self.bn2(self.conv2(x)) )\n",
    "    print(x.shape)\n",
    "    x = F.relu( self.bn3(self.conv3(x)) )\n",
    "    print(x.shape)\n",
    "    x = F.relu( self.bn4(self.conv4(x)) )\n",
    "    print(x.shape)\n",
    "    x = torch.tanh( self.conv5(x) )\n",
    "    print(x.shape)\n",
    "    return x\n",
    "    \n",
    "\n",
    "gnet = generatorNet()\n",
    "y = gnet(torch.randn(10,100,1,1))\n",
    "# print(y.shape)\n",
    "plt.imshow(y[0,:,:,:].squeeze().detach().numpy());"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSG36JfY_HJF"
   },
   "source": [
    "# Train the models!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SFDbnRqeCPqy"
   },
   "source": [
    "lossfun = nn.BCELoss()\n",
    "\n",
    "dnet = discriminatorNet().to(device)\n",
    "gnet = generatorNet().to(device)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(dnet.parameters(), lr=.0002, betas=(.5,.999))\n",
    "g_optimizer = torch.optim.Adam(gnet.parameters(), lr=.0002, betas=(.5,.999))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "83Ju8fDuUTBg"
   },
   "source": [
    "# training parameters and initializations\n",
    "num_epochs = 1500\n",
    "batchsize  = 86\n",
    "losses     = []\n",
    "disDecs    = []\n",
    "\n",
    "\n",
    "for epochi in range(num_epochs):\n",
    "\n",
    "  # create a minibatch from randomly selected images \n",
    "  ridx = torch.randint(images.shape[0],(batchsize,))\n",
    "  data = images[ridx,:].to(device)\n",
    "\n",
    "\n",
    "  # create labels for real and fake images\n",
    "  real_labels = torch.ones(batchsize,1).to(device)\n",
    "  fake_labels = torch.zeros(batchsize,1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "  ### ---------------- Train the discriminator ---------------- ###\n",
    "\n",
    "  # forward pass and loss for REAL pictures\n",
    "  pred_real   = dnet(data)                     # output of discriminator\n",
    "  d_loss_real = lossfun(pred_real,real_labels) # all labels are 1\n",
    "\n",
    "  # forward pass and loss for FAKE pictures\n",
    "  fake_data   = torch.randn(batchsize,100,1,1).to(device) # random numbers to seed the generator\n",
    "  fake_images = gnet(fake_data)                           # output of generator\n",
    "  pred_fake   = dnet(fake_images)                         # pass through discriminator\n",
    "  d_loss_fake = lossfun(pred_fake,fake_labels)            # all labels are 0\n",
    "\n",
    "  # collect loss (using combined losses)\n",
    "  d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "  # backprop\n",
    "  d_optimizer.zero_grad()\n",
    "  d_loss.backward()\n",
    "  d_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "  ### ---------------- Train the generator ---------------- ###\n",
    "\n",
    "  # create fake images and compute loss\n",
    "  fake_images = gnet( torch.randn(batchsize,100,1,1).to(device) )\n",
    "  pred_fake   = dnet(fake_images)\n",
    "\n",
    "  # compute loss\n",
    "  g_loss = lossfun(pred_fake,real_labels)\n",
    "\n",
    "  # backprop\n",
    "  g_optimizer.zero_grad()\n",
    "  g_loss.backward()\n",
    "  g_optimizer.step()\n",
    "\n",
    "\n",
    "  # collect losses and discriminator decisions\n",
    "  losses.append([d_loss.item(),g_loss.item()])\n",
    "  \n",
    "  d1 = torch.mean((pred_real>.5).float()).detach()\n",
    "  d2 = torch.mean((pred_fake>.5).float()).detach()\n",
    "  disDecs.append([d1,d2])\n",
    "\n",
    "\n",
    "  # print out a status message\n",
    "  if (epochi+1)%50==0:\n",
    "    msg = f'Finished epoch {epochi+1}/{num_epochs}'\n",
    "    sys.stdout.write('\\r' + msg)\n",
    "\n",
    "\n",
    "# convert performance from list to numpy array\n",
    "losses  = np.array(losses)\n",
    "disDecs = np.array(disDecs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xDZ3fTP3BUPq"
   },
   "source": [
    "# create a 1D smoothing filter\n",
    "def smooth(x,k=15):\n",
    "  return np.convolve(x,np.ones(k)/k,mode='same')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1C0qAf9kN7mi"
   },
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(18,5))\n",
    "\n",
    "ax[0].plot(smooth(losses[:,0]))\n",
    "ax[0].plot(smooth(losses[:,1]))\n",
    "ax[0].set_xlabel('Batches')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Model loss')\n",
    "ax[0].legend(['Discrimator','Generator'])\n",
    "# ax[0].set_xlim([500,900])\n",
    "# ax[0].set_ylim([0,2.5])\n",
    "\n",
    "ax[1].plot(losses[200:,0],losses[200:,1],'k.',alpha=.1)\n",
    "ax[1].set_xlabel('Discriminator loss')\n",
    "ax[1].set_ylabel('Generator loss')\n",
    "\n",
    "ax[2].plot(smooth(disDecs[:,0]))\n",
    "ax[2].plot(smooth(disDecs[:,1]))\n",
    "ax[2].set_xlabel('Epochs')\n",
    "ax[2].set_ylabel('Probablity (\"real\")')\n",
    "ax[2].set_title('Discriminator output')\n",
    "ax[2].legend(['Real','Fake'])\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElnXz0ZkS8Yc"
   },
   "source": [
    "# Let's see some generated Gaussians!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AzCz1UqGCP8T"
   },
   "source": [
    "# generate the images from the generator network\n",
    "gnet.eval()\n",
    "fake_data = gnet( torch.randn(batchsize,100,1,1).to(device) ).cpu()\n",
    "\n",
    "# and visualize...\n",
    "fig,axs = plt.subplots(3,6,figsize=(12,6))\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "  ax.imshow(fake_data[i,].detach().squeeze(),cmap='jet')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N3p-fkwJyL28"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_oHZ3xSNAX3"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V1w-vJlJNB__"
   },
   "source": [
    "# 1) Train the model for 200 epochs. Do the generated Gaussians still look like Gaussians?\n",
    "# \n",
    "# 2) The images are set to 64x64. Does the model still work if you make the images larger, e.g., 91x91? How about if\n",
    "#    they are smaller, e.g., 28x28 (the size of MNIST)?\n",
    "# "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}