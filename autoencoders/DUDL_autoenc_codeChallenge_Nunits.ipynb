{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c12b53",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ydebessu/DeepUnderstandingOfDeepLearning/blob/main/autoencoders/DUDL_autoenc_codeChallenge_Nunits.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: CodeChallenge: how many units?\n",
    "### LECTURE: Denoising MNIST\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# NEW!\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HOkOefftqyg"
   },
   "source": [
    "# Import and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MU7rvmWuhjud"
   },
   "outputs": [],
   "source": [
    "# import dataset (comes with colab!)\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# don't need labels!\n",
    "data = data[:,1:]\n",
    "\n",
    "# normalize the data to a range of [0 1]\n",
    "dataNorm = data / np.max(data)\n",
    "\n",
    "# convert to tensor\n",
    "dataT = torch.tensor( dataNorm ).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK8Opkhgp0bO"
   },
   "source": [
    "# Create the DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JK3OO3tAtZkA"
   },
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "def createTheMNISTAE(n_enc,n_bottle):\n",
    "\n",
    "  class aenet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      ### input layer\n",
    "      self.input = nn.Linear(784,n_enc)\n",
    "      \n",
    "      ### encoder layer\n",
    "      self.encoding = nn.Linear(n_enc,n_bottle)\n",
    "\n",
    "      ### bottleneck layer\n",
    "      self.bottleneck = nn.Linear(n_bottle,n_enc)\n",
    "\n",
    "      ### decoder layer\n",
    "      self.decoding = nn.Linear(n_enc,784)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x = F.relu( self.input(x) )\n",
    "      x = F.relu( self.encoding(x) )\n",
    "      x = F.relu( self.bottleneck(x) )\n",
    "      y = torch.sigmoid( self.decoding(x) )\n",
    "      return y\n",
    "  \n",
    "  # create the model instance\n",
    "  net = aenet()\n",
    "  \n",
    "  # loss function\n",
    "  lossfun = nn.MSELoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.Adam(net.parameters(),lr=.001)\n",
    "\n",
    "  return net,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvfGQIRGp0ht"
   },
   "source": [
    "# Create a function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IblJo1NCp0kl"
   },
   "outputs": [],
   "source": [
    "# note the difference in selecting samples compared to DUDL_autoenc_denoisingMNIST\n",
    "\n",
    "def function2trainTheModel(n_enc,n_bottle):\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 3\n",
    "  \n",
    "  # create a new model\n",
    "  net,lossfun,optimizer = createTheMNISTAE(n_enc,n_bottle)\n",
    "\n",
    "  # initialize losses\n",
    "  losses = []\n",
    "\n",
    "\n",
    "  # batch size and number of batches\n",
    "  batchsize  = 32\n",
    "  numBatches = int(dataT.shape[0]/batchsize)\n",
    "\n",
    "\n",
    "  # loop over epochs (now each epoch goes through all samples)\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    \n",
    "    # get a permuted index vector\n",
    "    randidx = np.random.permutation(dataT.shape[0]).astype(int)\n",
    "\n",
    "    # losses during the batches\n",
    "    batchlosses = []\n",
    "\n",
    "    for batchi in range(numBatches):\n",
    "      \n",
    "      # samples to use in this batch\n",
    "      samps2use = range((batchi-1)*batchsize,batchi*batchsize)\n",
    "      \n",
    "\n",
    "      # select those images\n",
    "      X = dataT[randidx[samps2use],:]\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "      loss = lossfun(yHat,X)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    \n",
    "      # losses in this batch\n",
    "      batchlosses.append( loss.item() )\n",
    "    # end minibatch loop\n",
    "  \n",
    "    losses.append( np.mean(batchlosses[-3:]) )\n",
    "    # Note about the above line: This was slightly incorrect in the recording. I said\n",
    "    # to average the last 3 training losses, but the code in the video averaged all\n",
    "    # the batch losses. However, that doesn't change the outcome or the conclusions.\n",
    "  # end epoch loop\n",
    "  \n",
    "  # function output\n",
    "  return losses,net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpGm9xdQ27Ob"
   },
   "source": [
    "# Run the experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9pCC1R2p0nu"
   },
   "outputs": [],
   "source": [
    "# note! this cell takes ~25 minutes with initial N settings\n",
    "\n",
    "# specific the number of units\n",
    "N_encdec_units = np.linspace(10,500,12).astype(int)\n",
    "N_bottle_units = np.linspace(5,100,8).astype(int)\n",
    "\n",
    "\n",
    "# initialize results matrix\n",
    "exp_results = np.zeros((len(N_encdec_units),len(N_bottle_units)))\n",
    "\n",
    "\n",
    "# start the experiment!\n",
    "for ei,nenc in enumerate(N_encdec_units):\n",
    "  for bi,nbot in enumerate(N_bottle_units):\n",
    "\n",
    "    # build/train a model\n",
    "    losses = function2trainTheModel(nenc,nbot)[0] # only need the first output\n",
    "    exp_results[ei,bi] = np.mean(losses[-1])\n",
    "\n",
    "    # send update message\n",
    "    currentIter = ei*len(N_bottle_units)+bi+1\n",
    "    totalIters  = len(N_bottle_units)*len(N_encdec_units)\n",
    "    msg = 'Finished experiment {}/{}'.format(currentIter,totalIters)\n",
    "    sys.stdout.write('\\r' + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vt2VXlYJX5Gj"
   },
   "outputs": [],
   "source": [
    "# show the results matrix\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.imshow(exp_results,aspect='auto',         # data and aspect ratio\n",
    "           vmin=.01,vmax=.04, cmap='Purples', # color range and palette\n",
    "           extent=[ N_bottle_units[0],N_bottle_units[-1],N_encdec_units[-1],N_encdec_units[0], ]) # xy axis ticks\n",
    "           \n",
    "\n",
    "plt.xlabel('Number of bottleneck units')\n",
    "plt.ylabel('Number of encoder/decoder units')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SnUUHPm7xQE"
   },
   "outputs": [],
   "source": [
    "# perhaps a line plot will better reveal the sudden transition\n",
    "\n",
    "plt.plot(N_encdec_units,exp_results)\n",
    "plt.legend(N_bottle_units,loc=(1.01,0))\n",
    "plt.xlabel('Number of enc/dec units')\n",
    "plt.title('Loss by bottleneck units')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbEObe0N95k4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh28k_l29urR"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ib3uQtfv9wE2"
   },
   "outputs": [],
   "source": [
    "# 1) Because the full experiment takes a long time, it's not pratical to add another factor. Fix the number of encoder\n",
    "#    units to 100 and instead parametrically explore the learning rate. You don't need so many learning rates, just use\n",
    "#    [.0001, .001, .01]. The results can be shown in a line plot, with one line per lr and bottleneck units on the x-axis.\n",
    "# \n",
    "# 2) Smooth transitions across parameters are easy to interpret. But the image plot shows a sharp transition for small\n",
    "#    numbers of bottleneck units. This rings alarm bells for any experimental scientist! It means that something is \n",
    "#    happening at that region of parameter space and you should investigate. Thus, re-run the experiment but change the\n",
    "#    parameters to focus specifically on the region of the parameter space where there are large changes in the results.\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNcutK9SxFxddJz4tNztx58",
   "collapsed_sections": [],
   "name": "DUDL_autoenc_codeChallenge_Nunits.ipynb",
   "provenance": [
    {
     "file_id": "19G9gTeBlYPQ-s3VS_3K2bVFtKTP344j6",
     "timestamp": 1619192263464
    },
    {
     "file_id": "1FcEBC0NAESIlHQkv6_85R-XDUKGE8XbM",
     "timestamp": 1619155961717
    },
    {
     "file_id": "1qKgZ8kVcqNgwtBzHbWq5yJH_HqI6DxWW",
     "timestamp": 1617803880910
    },
    {
     "file_id": "15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4",
     "timestamp": 1617737766196
    },
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617734878578
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
