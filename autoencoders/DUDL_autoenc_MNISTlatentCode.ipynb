{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DUDL_autoenc_MNISTlatentCode.ipynb",
   "provenance": [
    {
     "file_id": "19G9gTeBlYPQ-s3VS_3K2bVFtKTP344j6",
     "timestamp": 1619266919349
    },
    {
     "file_id": "1FcEBC0NAESIlHQkv6_85R-XDUKGE8XbM",
     "timestamp": 1619155961717
    },
    {
     "file_id": "1qKgZ8kVcqNgwtBzHbWq5yJH_HqI6DxWW",
     "timestamp": 1617803880910
    },
    {
     "file_id": "15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4",
     "timestamp": 1617737766196
    },
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617734878578
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyO+XbpZzuIwe31qEBui38Be"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Autoencoders\n",
    "### LECTURE: The latent code of MNIST\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# NEW! for doing PCA on the model output\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HOkOefftqyg"
   },
   "source": [
    "# Import and process the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MU7rvmWuhjud"
   },
   "source": [
    "# import dataset (comes with colab!)\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# we'll use the labels for matching with the latent code\n",
    "labels = data[:,0]\n",
    "data   = data[:,1:]\n",
    "\n",
    "# normalize the data to a range of [0 1]\n",
    "dataNorm = data / np.max(data)\n",
    "\n",
    "# convert to tensor\n",
    "dataT = torch.tensor( dataNorm ).float()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK8Opkhgp0bO"
   },
   "source": [
    "# Create the DL model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JK3OO3tAtZkA"
   },
   "source": [
    "# create a class for the model\n",
    "def createTheMNISTAE():\n",
    "\n",
    "  class aenet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      ### input layer\n",
    "      self.input = nn.Linear(784,150)\n",
    "      \n",
    "      ### encoder layer\n",
    "      self.enc = nn.Linear(150,15)\n",
    "\n",
    "      ### latent layer\n",
    "      self.lat = nn.Linear(15,150)\n",
    "\n",
    "      ### decoder layer\n",
    "      self.dec = nn.Linear(150,784)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x = F.relu( self.input(x) )\n",
    "      \n",
    "      # NEW! output the hidden-layer activation\n",
    "      codex = F.relu( self.enc(x) )\n",
    "      \n",
    "      x = F.relu( self.lat(codex) )\n",
    "      y = torch.sigmoid( self.dec(x) )\n",
    "      return y,codex\n",
    "  \n",
    "  # create the model instance\n",
    "  net = aenet()\n",
    "  \n",
    "  # loss function\n",
    "  lossfun = nn.MSELoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.Adam(net.parameters(),lr=.001)\n",
    "\n",
    "  return net,lossfun,optimizer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "voQ6mHkfmj1F"
   },
   "source": [
    "# test the model with a bit of data\n",
    "net,lossfun,optimizer = createTheMNISTAE()\n",
    "\n",
    "X = dataT[:5,:]\n",
    "yHat = net(X)\n",
    "\n",
    "print('Input shape:')\n",
    "print(X.shape)\n",
    "print(' ')\n",
    "\n",
    "# yHat is now a tuple\n",
    "print(type(yHat),len(yHat))\n",
    "print(' ')\n",
    "\n",
    "print('Shape of model output:')\n",
    "print(yHat[0].shape)\n",
    "print(' ')\n",
    "\n",
    "print('Shape of encoding layer output:')\n",
    "print(yHat[1].shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvfGQIRGp0ht"
   },
   "source": [
    "# Create a function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IblJo1NCp0kl"
   },
   "source": [
    "def function2trainTheModel():\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 10000\n",
    "  \n",
    "  # create a new model\n",
    "  net,lossfun,optimizer = createTheMNISTAE()\n",
    "\n",
    "  # initialize losses\n",
    "  losses = torch.zeros(numepochs)\n",
    "\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # select a random set of images\n",
    "    randomidx = np.random.choice(dataT.shape[0],size=32)\n",
    "    X = dataT[randomidx,:]\n",
    "\n",
    "    # forward pass and loss\n",
    "    yHat = net(X)[0] # NEW! here we only care about the final model output\n",
    "    loss = lossfun(yHat,X)\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # losses in this epoch\n",
    "    losses[epochi] = loss.item()\n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return losses,net"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpGm9xdQ27Ob"
   },
   "source": [
    "# Run the model and show the results!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l9pCC1R2p0nu"
   },
   "source": [
    "# train the model\n",
    "losses,net = function2trainTheModel()\n",
    "print(f'Final loss: {losses[-1]:.4f}')\n",
    "\n",
    "# visualize the losses\n",
    "plt.plot(losses,'.-')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Model loss')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG7_3tYbp0wm"
   },
   "source": [
    "# Inspect the latent \"code\" of the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qw56zhmj87WC"
   },
   "source": [
    "# output the latent layer\n",
    "\n",
    "# push through the entire dataset\n",
    "yHat,latent = net(dataT)\n",
    "\n",
    "# print sizes\n",
    "print(yHat.shape)\n",
    "print(latent.shape)\n",
    "\n",
    "# what does it look like?\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].hist(latent.flatten().detach(),100)\n",
    "ax[0].set_xlabel('Latent activation value')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_title('Distribution of latent units activations')\n",
    "\n",
    "ax[1].imshow(latent.detach(),aspect='auto',vmin=0,vmax=10)\n",
    "ax[1].set_xlabel('Latent node')\n",
    "ax[1].set_ylabel('Image number')\n",
    "ax[1].set_title('All latent activations')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9SnUUHPm7xQE"
   },
   "source": [
    "# compute the average latent activation for each digit type\n",
    "\n",
    "# initialize output matrix (latent shape by 10 digits)\n",
    "sourcecode = np.zeros((latent.shape[1],10))\n",
    "\n",
    "\n",
    "# loop over digit categories\n",
    "for i in range(10):\n",
    "\n",
    "  # find all pictures of this category\n",
    "  digidx = np.where(labels==i)\n",
    "\n",
    "  # average the latent layer output\n",
    "  sourcecode[:,i] = torch.mean(latent[digidx,:],axis=1).detach()\n",
    "\n",
    "\n",
    "# let's see what it looks like!\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.plot(sourcecode,'s-')\n",
    "plt.legend(range(10),loc=(1.01,.4))\n",
    "plt.xticks(range(15))\n",
    "plt.xlabel('Latent node number')\n",
    "plt.ylabel('Activation')\n",
    "plt.title(\"The model's internal representation of the numbers\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RumWkrM82B8N"
   },
   "source": [
    "# Explore the reduced-compressed space with PCA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mRrEQMXgrCQj"
   },
   "source": [
    "# compute and fit the PCA\n",
    "pcaData = PCA(n_components=15).fit(data) # 15 components to match latent, but it's just to speed computation time\n",
    "pcaCode = PCA(               ).fit(latent.detach())\n",
    "\n",
    "\n",
    "# plot the eigenspectra (scree plot)\n",
    "plt.plot(100*pcaData.explained_variance_ratio_,'s-',label='Data PCA')\n",
    "plt.plot(100*pcaCode.explained_variance_ratio_,'o-',label='Code PCA')\n",
    "plt.xlabel('Components')\n",
    "plt.ylabel('Percent variance explained')\n",
    "plt.title('PCA scree plot')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9cAVDrS5vbWC"
   },
   "source": [
    "# compute the projection of the data onto the PC axes\n",
    "scoresData = pcaData.fit_transform(data)\n",
    "scoresCode = pcaCode.fit_transform(latent.detach())\n",
    "\n",
    "# plot the data separately per number\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "for lab in range(10):\n",
    "  ax[0].plot(scoresData[labels==lab,0],scoresData[labels==lab,1],'o',markersize=3,alpha=.4)\n",
    "  ax[1].plot(scoresCode[labels==lab,0],scoresCode[labels==lab,1],'o',markersize=3,alpha=.4)\n",
    "\n",
    "for i in range(2):\n",
    "  ax[i].set_xlabel('PC1 projection')\n",
    "  ax[i].set_ylabel('PC2 projection')\n",
    "  ax[i].legend(range(10))\n",
    "\n",
    "ax[0].set_title('PCA of data')\n",
    "ax[1].set_title('PCA of latent code')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3vB1b_MGBlVv"
   },
   "source": [
    "# This cell is not important! It's just the code I used to make the figure in the slide. I decided to leave it here FYI.\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(15,3))\n",
    "\n",
    "ax[0].imshow(dataT[0,:].view(28,28),cmap='gray')\n",
    "\n",
    "ax[1].plot(dataT[0,:],'ks')\n",
    "ax[1].set_xlabel('Pixels (vectorized)')\n",
    "ax[1].set_ylabel('Intensity value')\n",
    "\n",
    "ax[2].plot(latent[0,:].detach(),'ks')\n",
    "ax[2].set_xlabel('Latent units')\n",
    "ax[2].set_ylabel('Activation (a.u.)')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nlmKG35AVGJp"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh28k_l29urR"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ib3uQtfv9wE2"
   },
   "source": [
    "# 1) Are you surprised that the latent activations (e.g., from the histogram) are all non-negative? Is that because of \n",
    "#    the image normalization, or what is causing those values to be all non-negative?\n",
    "# \n",
    "# 2) Averages don't tell the whole story. Redraw the \"Model's internal representation\" line plot but using standard \n",
    "#    deviation instead of mean. This graph will tell you if any numbers, or units, have particularly higher variability\n",
    "#    than others. Is this the case, and does the std plot give you any more insight into the model's learned representation?\n",
    "# \n",
    "# 3) The PC-space plots are tricky to interpret: This is a 15-dimensional space but 13 dimensions are projected onto two.\n",
    "#    It's possible that the numbers are better separated in other dimensions, just like a 2D photograph of someone standing\n",
    "#    behind a tree makes them inseparable whereas they are separable in the original 3D space. Modify the plot to show\n",
    "#    PC dimensions 2&3 instead of 1&2. \n",
    "# "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}