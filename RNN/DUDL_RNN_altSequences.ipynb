{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DUDL_RNN_altSequences.ipynb",
   "provenance": [
    {
     "file_id": "19WUFNKOHKnZ1hEkR6havrl_eIE4BxuaE",
     "timestamp": 1621268036592
    },
    {
     "file_id": "1BI9p-vnVoi7Tm8yiQgVN3kpM2VuEzfeE",
     "timestamp": 1621245811372
    },
    {
     "file_id": "1o_dLKV6fY7xdZYx_pNMY12zpL_pmMurs",
     "timestamp": 1618865813618
    },
    {
     "file_id": "1Q9LtmanyNt675-gO_kXRBKalCdP6xtvV",
     "timestamp": 1617253457100
    },
    {
     "file_id": "1jeqKEJfI18GlAhSG8RO5aJ6Vrp4-nkTt",
     "timestamp": 1615909315432
    },
    {
     "file_id": "10_geQnah5AvMsm8VDAQwNPhypOXradar",
     "timestamp": 1615893340208
    },
    {
     "file_id": "1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD",
     "timestamp": 1615877547147
    }
   ],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyOPD7DQOJ9re8ygNIq6i8lv"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: RNNs (and LSTM and GRU)\n",
    "### LECTURE: Predicting alternating sequences\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j7-LiwqUMGYL"
   },
   "source": [
    "### import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# for printing out status reports\n",
    "import sys\n",
    "\n",
    "# for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2anVFzBXGdwH"
   },
   "source": [
    "# Create temporal sequence data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0ohXIxzt4_U2"
   },
   "source": [
    "# import the data\n",
    "N = 50\n",
    "\n",
    "data = torch.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "  data[i] = torch.rand(1) * (-1)**i\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.plot([-1,N+1],[0,0],'--',color=[.8,.8,.8])\n",
    "plt.plot(data,'ks-',markerfacecolor='w')\n",
    "plt.xlim([-1,N+1])\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VToReEHNWP0n"
   },
   "source": [
    "# Create a class for the DL model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RVcrKOC1Wqk-"
   },
   "source": [
    "class rnnnet(nn.Module):\n",
    "  def __init__(self,input_size,num_hidden,num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    # RNN Layer\n",
    "    self.rnn = nn.RNN(input_size,num_hidden,num_layers)\n",
    "    \n",
    "    # linear layer for output\n",
    "    self.out = nn.Linear(num_hidden,1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    \n",
    "    # run through the RNN layer\n",
    "    y,hidden = self.rnn(x) # no explicit hidden state initialization\n",
    "    \n",
    "    # and the output (linear) layer\n",
    "    y = self.out(y)\n",
    "    \n",
    "    return y,hidden"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zQKkLPUeWqn_"
   },
   "source": [
    "# network parameters\n",
    "input_size =  1 # \"channels\" of data\n",
    "num_hidden =  5 # breadth of model (number of units in hidden layers)\n",
    "num_layers =  1 # depth of model (number of \"stacks\" of hidden layers)\n",
    "seqlength  =  9 # number of datapoints used for learning in each segment\n",
    "batchsize  =  1 # Note: the training code is actually hard-coded to organize data into batchsize=1\n",
    "\n",
    "# create an instance of the model and inspect\n",
    "net = rnnnet(input_size,num_hidden,num_layers)\n",
    "\n",
    "X = torch.rand(seqlength,batchsize,input_size)\n",
    "y,h = net(X)\n",
    "print(X.shape)\n",
    "print(y.shape) # note: one output per sequence element; generally, we take the final output to force a \"many-to-one\" design.\n",
    "print(h.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0WL8wjwn0Jwr"
   },
   "source": [
    "# test the model with some data\n",
    "somedata = data[:seqlength].view(seqlength,1,1)\n",
    "y = net(somedata)\n",
    "\n",
    "# grab the final predicted value from the output (first element of tuple output of net)\n",
    "finalValue = y[0][-1]\n",
    "\n",
    "lossfun = nn.MSELoss()\n",
    "lossfun(finalValue,data[seqlength].view(1,1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUMPfhoFWqrA"
   },
   "source": [
    "# Train the model and show performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4aUKZKn5Wqt-"
   },
   "source": [
    "# number of training epochs\n",
    "numepochs = 30\n",
    "\n",
    "# create a new instance of the model (and optimizer!)\n",
    "net = rnnnet(input_size,num_hidden,num_layers)\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=.001)\n",
    "\n",
    "\n",
    "\n",
    "# initialize losses\n",
    "losses = np.zeros(numepochs)\n",
    "signaccuracy = np.zeros(numepochs)\n",
    "\n",
    "# loop over epochs\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "  # loop over data segments\n",
    "  seglosses = []\n",
    "  segacc    = []\n",
    "  hidden_state = torch.zeros(num_layers,batchsize,num_hidden) # reset the hidden state on each epoch\n",
    "\n",
    "  for timei in range(N-seqlength):\n",
    "\n",
    "    # grab a snippet of data\n",
    "    X = data[timei:timei+seqlength].view(seqlength,1,1)\n",
    "    y = data[timei+seqlength].view(1,1)\n",
    "\n",
    "    # forward pass and loss\n",
    "    yHat,hidden_state = net(X)\n",
    "    finalValue = yHat[-1]\n",
    "    loss = lossfun(finalValue,y) # compare final value of output\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # loss from this segment\n",
    "    seglosses.append(loss.item())\n",
    "\n",
    "    # also get sign accuracy\n",
    "    truesign = np.sign(torch.squeeze(y).numpy())\n",
    "    predsign = np.sign(torch.squeeze(finalValue).detach().numpy())\n",
    "    accuracy = 100*(truesign==predsign)\n",
    "    segacc.append(accuracy)\n",
    "  \n",
    "  # average losses from this epoch\n",
    "  losses[epochi] = np.mean(seglosses)\n",
    "  signaccuracy[epochi] = np.mean(segacc)\n",
    "  \n",
    "  msg = f'Finished epoch {epochi+1}/{numepochs}'\n",
    "  sys.stdout.write('\\r' + msg)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eTf-tsC7Iuot"
   },
   "source": [
    "truesign"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V_Au7fIUWq0H"
   },
   "source": [
    "## let's see how the model did!\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
    "\n",
    "ax[0].plot(losses,'s-')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Model loss')\n",
    "\n",
    "ax[1].plot(signaccuracy,'m^-',markerfacecolor='g',markersize=15)\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Sign accuracy (final accuracy: %.2f%%)'%signaccuracy[-1])\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB4GbxkSWrK-"
   },
   "source": [
    "# Now test the network!\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b6Rq17IVD12h"
   },
   "source": [
    "# initialize hidden state\n",
    "h = np.zeros((N,num_hidden))\n",
    "\n",
    "# initialize predicted values\n",
    "yHat = np.zeros(N)\n",
    "yHat[:] = np.nan\n",
    "\n",
    "\n",
    "# loop over time segments\n",
    "for timei in range(N-seqlength):\n",
    "\n",
    "  # grab a snippet of data\n",
    "  X = data[timei:timei+seqlength].view(seqlength,1,1)\n",
    "\n",
    "  # forward pass and loss\n",
    "  yy,hh = net(X)\n",
    "  yHat[timei+seqlength] = yy[-1]\n",
    "  h[timei+seqlength,:] = hh.detach()\n",
    "\n",
    "\n",
    "## compute sign-accuracy\n",
    "truesign = np.sign(data.numpy())\n",
    "predsign = np.sign(yHat)\n",
    "signaccuracy = 100*np.mean(truesign[seqlength:]==predsign[seqlength:])\n",
    "\n",
    "\n",
    "## plot!\n",
    "fig,ax = plt.subplots(1,3,figsize=(16,4))\n",
    "ax[0].plot(data,'bs-',label='Actual data')\n",
    "ax[0].plot(yHat,'ro-',label='Predicted')\n",
    "ax[0].set_ylim([-1.1,1.1])\n",
    "ax[0].set_title('Sign accuracy = %.2f%%' %signaccuracy)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(data-yHat,'k^')\n",
    "ax[1].set_ylim([-1.1,1.1])\n",
    "\n",
    "ax[2].plot(data[seqlength:],yHat[seqlength:],'mo')\n",
    "ax[2].set_xlabel('Real data')\n",
    "ax[2].set_ylabel('Predicted data')\n",
    "r = np.corrcoef(data[seqlength:],yHat[seqlength:])\n",
    "ax[2].set_title(f\"r={r[0,1]:.2f} (but Simpson's paradox!)\")\n",
    "\n",
    "plt.suptitle('Performance on training data',fontweight='bold',fontsize=20,y=1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rE0uHldnpoar"
   },
   "source": [
    "# show the hidden \"states\" (units activations)\n",
    "plt.figure(figsize=(16,5))\n",
    "\n",
    "plt.plot(h,'s-')\n",
    "plt.xlabel('Sequence index')\n",
    "plt.ylabel('State value (a.u.)')\n",
    "plt.title('Each line is a different hidden unit')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLH6POAU_5sy"
   },
   "source": [
    "# Test with new data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J7q4jjp__7G0"
   },
   "source": [
    "# Create new data WITH FLIPPED SIGNS!\n",
    "newdata = torch.zeros(N)\n",
    "for i in range(N):\n",
    "  newdata[i] = torch.rand(1) * (-1)**(i+1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MYMSUJq6_7J1"
   },
   "source": [
    "## now test the network!\n",
    "# note: no learning here!\n",
    "\n",
    "h = np.zeros((N,num_hidden))\n",
    "\n",
    "yHat = np.zeros(N)\n",
    "for timei in range(N-seqlength):\n",
    "\n",
    "  # grab a snippet of data\n",
    "  X = newdata[timei:timei+seqlength].view(seqlength,1,1)\n",
    "\n",
    "  # forward pass and loss\n",
    "  yy,hh = net(X)\n",
    "  yHat[timei+seqlength] = yy[-1]\n",
    "  h[timei+seqlength,:] = hh.detach()\n",
    "\n",
    "\n",
    "# compute sign-accuracy\n",
    "truesign = np.sign(newdata.numpy())\n",
    "predsign = np.sign(yHat)\n",
    "signaccuracy = 100*np.mean(truesign[seqlength:]==predsign[seqlength:])\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(18,4))\n",
    "ax[0].plot(newdata,'bs-',label='Actual data')\n",
    "ax[0].plot(yHat,'ro-',label='Predicted')\n",
    "ax[0].set_ylim([-1.1,1.1])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(newdata-yHat,'k^')\n",
    "ax[1].set_ylim([-1.1,1.1])\n",
    "ax[1].set_title('Sign accuracy = %.2f%%' %signaccuracy)\n",
    "\n",
    "plt.suptitle('Performance on unseen test data',fontweight='bold',fontsize=20,y=1.1)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L9x4fwIJ_7M0"
   },
   "source": [
    "# plot the weights for the input->hidden layers\n",
    "plt.bar(range(num_hidden),net.rnn.weight_ih_l0.detach())\n",
    "plt.ylabel('Weight value')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XXtU8uw-_7P0"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdQ9d5UC_7Ss"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J9EweIKOI1td"
   },
   "source": [
    "# 1) Is the model overfitting? One way to check is by setting the signs to be random instead of alternating. You can do \n",
    "#    this by modifying the data-generation code to normal random numbers without forcing the sign. What is the predicted\n",
    "#    accuracy level in this case?\n",
    "# \n",
    "# 2) The hidden state is typically initialized to zeros. Is that really the best initialization? Weights are initialized\n",
    "#    to random numbers. What happens if you initialize the hidden state to randn()? Run the model several times to get a\n",
    "#    sense of the general trends. Now try initializing to all 100 (instead of zeros). Why are you getting these results?\n",
    "# \n",
    "# 3) The data problem here (predicting alternating signs) is pretty easy. Would this same model architecture perform as\n",
    "#    well for more complicated sequences? As a start, modify the data-generating code to have the sequence ++- (thus, \n",
    "#    two positive numbers and a negative number, then repeat that sign-pattern over and over again). Once you have this\n",
    "#    code, you can test a variety of sign-sequencies, like ++--, --+, ++---, etc. Lots of fun to be had ;)\n",
    "# \n",
    "# 4) I set the sequence length to be 9. Do you think that's a good value here? Of course, this is a metaparameter that\n",
    "#    you can pick, and the exact numerical value is a bit arbitrary. But some values are more appropriate than other\n",
    "#    values, depending on the nature of the data. Based on what you know about our sequence data, what do you think about\n",
    "#    the value of 9? What sequence lengths would be appropriate for the suggested sequences in question #3? \n",
    "# "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s7ioUURaI1zN"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}