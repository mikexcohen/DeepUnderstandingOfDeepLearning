{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhWV8oes-wKR"
      },
      "source": [
        "# COURSE: A deep understanding of deep learning\n",
        "## SECTION: More on data\n",
        "### LECTURE: Sample size and network depth\n",
        "#### TEACHER: Mike X Cohen, sincxpress.com\n",
        "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202305"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7-LiwqUMGYL"
      },
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline.backend_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-SP8NPsMNRL"
      },
      "source": [
        "# a function that creates data\n",
        "\n",
        "def createSomeData(nPerClust):\n",
        "\n",
        "  A = [ 1, 1 ]\n",
        "  B = [ 5, 1 ]\n",
        "  C = [ 4, 4 ]\n",
        "\n",
        "  # generate data\n",
        "  a = [ A[0]+np.random.randn(nPerClust) , A[1]+np.random.randn(nPerClust) ]\n",
        "  b = [ B[0]+np.random.randn(nPerClust) , B[1]+np.random.randn(nPerClust) ]\n",
        "  c = [ C[0]+np.random.randn(nPerClust) , C[1]+np.random.randn(nPerClust) ]\n",
        "\n",
        "  # true labels\n",
        "  labels_np = np.hstack(( np.zeros((nPerClust)),\n",
        "                          np.ones( (nPerClust)),\n",
        "                        1+np.ones( (nPerClust))  ))\n",
        "\n",
        "  # concatanate into a matrix, then convert to a pytorch tensor\n",
        "  data_np = np.hstack((a,b,c)).T\n",
        "\n",
        "  # NEW: put all outputs into a dictionary\n",
        "  output = {}\n",
        "  output['data'] = torch.tensor(data_np).float()\n",
        "  output['labels'] = torch.tensor(labels_np).long() # note: \"long\" format (integers) for labels\n",
        "\n",
        "  # use scikitlearn to split the data\n",
        "  train_data,test_data, train_labels,test_labels = train_test_split(output['data'], output['labels'], train_size=.9)\n",
        "\n",
        "  # then convert them into PyTorch Datasets (note: already converted to tensors)\n",
        "  train_data = TensorDataset(train_data,train_labels)\n",
        "  test_data  = TensorDataset(test_data,test_labels)\n",
        "\n",
        "  # finally, translate into dataloader objects\n",
        "  batchsize  = 8\n",
        "  output['train_data'] = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "  output['test_data']  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMCgk85rPhB2"
      },
      "source": [
        "# Check that the function works\n",
        "\n",
        "theData = createSomeData(50)\n",
        "\n",
        "data = theData['data']\n",
        "labels = theData['labels']\n",
        "\n",
        "# show the data\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'bs',alpha=.5)\n",
        "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'ko',alpha=.5)\n",
        "plt.plot(data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'r^',alpha=.5)\n",
        "plt.title('The qwerties!')\n",
        "plt.xlabel('qwerty dimension 1')\n",
        "plt.ylabel('qwerty dimension 2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36x0DIWe3RkB"
      },
      "source": [
        "# Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0YpD6f-j8dG"
      },
      "source": [
        "# create a class for the model\n",
        "def createTheQwertyNet(nUnits,nLayers):\n",
        "\n",
        "  class qwertyNet(nn.Module):\n",
        "    def __init__(self,nUnits,nLayers):\n",
        "      super().__init__()\n",
        "\n",
        "      # create dictionary to store the layers\n",
        "      self.layers = nn.ModuleDict()\n",
        "      self.nLayers = nLayers\n",
        "\n",
        "      ### input layer\n",
        "      self.layers['input'] = nn.Linear(2,nUnits)\n",
        "\n",
        "      ### hidden layers\n",
        "      for i in range(1,nLayers):\n",
        "        self.layers[f'hidden{i}'] = nn.Linear(nUnits,nUnits)\n",
        "\n",
        "      ### output layer\n",
        "      self.layers['output'] = nn.Linear(nUnits,3)\n",
        "\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self,x):\n",
        "      # input layer\n",
        "      x = self.layers['input'](x)\n",
        "\n",
        "      # hidden layers\n",
        "      for i in range(1,self.nLayers):\n",
        "        x = F.relu( self.layers[f'hidden{i}'](x) )\n",
        "\n",
        "      # return output layer\n",
        "      x = self.layers['output'](x)\n",
        "      return x\n",
        "\n",
        "  # create the model instance\n",
        "  net = qwertyNet(nUnits,nLayers)\n",
        "\n",
        "  # loss function\n",
        "  lossfun = nn.CrossEntropyLoss()\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = torch.optim.SGD(net.parameters(),lr=.01)\n",
        "\n",
        "  return net,lossfun,optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krQeh5wYMNla"
      },
      "source": [
        "# test the model with fake input\n",
        "\n",
        "nUnitsPerLayer = 12\n",
        "nLayers = 4\n",
        "\n",
        "net,lossf,opt = createTheQwertyNet(nUnitsPerLayer,nLayers)\n",
        "print(net)\n",
        "\n",
        "# input is ten samples\n",
        "input = torch.rand(10,2)\n",
        "net(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note:\n",
        "### There was a bug in the code I used in the video, which a student caught (see Q&A for this lecture).\n",
        "### This file is corrected, but it doesn't change the pattern or conclusion of the video.\n",
        "### The code cell below confirms that the model preserves the number of total units while varying the number of layers."
      ],
      "metadata": {
        "id": "TNKF2-H2qCU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a function to count the number of units\n",
        "def count_units(model):\n",
        "  total_units = 0\n",
        "  for layer in model.modules():\n",
        "    if isinstance(layer, torch.nn.modules.linear.Linear):\n",
        "      total_units += layer.in_features\n",
        "  return total_units\n",
        "\n",
        "\n",
        "nNodesInModel = 80\n",
        "layersRange   = [ 1,5,10,20 ]\n",
        "\n",
        "# print out the model architectures\n",
        "for lidx,layers in enumerate(layersRange):\n",
        "\n",
        "  # create a model\n",
        "  unitsperlayer = int(nNodesInModel//layersRange[lidx])\n",
        "  net = createTheQwertyNet(unitsperlayer,layers)[0]\n",
        "\n",
        "  # count and print the results\n",
        "  print(f'Units/layer: {unitsperlayer}, layers: {layers}, Total count: {count_units(net)}')"
      ],
      "metadata": {
        "id": "lA033nNriLvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4TW9zEu3Ueh"
      },
      "source": [
        "# A function that trains the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q0nmoUPmu-5"
      },
      "source": [
        "def function2trainTheModel(nUnits,nLayers):\n",
        "\n",
        "  # number of epochs\n",
        "  numepochs = 50\n",
        "\n",
        "  # create a new model\n",
        "  net,lossfun,optimizer = createTheQwertyNet(nUnits,nLayers)\n",
        "\n",
        "  # initialize losses\n",
        "  losses   = torch.zeros(numepochs)\n",
        "  trainAcc = []\n",
        "  testAcc  = []\n",
        "\n",
        "  # loop over epochs\n",
        "  for epochi in range(numepochs):\n",
        "\n",
        "    # loop over training data batches\n",
        "    batchAcc  = []\n",
        "    batchLoss = []\n",
        "    for X,y in train_data:\n",
        "\n",
        "      # forward pass and loss\n",
        "      yHat = net(X)\n",
        "      loss = lossfun(yHat,y)\n",
        "\n",
        "      # backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # loss from this batch\n",
        "      batchLoss.append(loss.item())\n",
        "\n",
        "      # compute accuracy\n",
        "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
        "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
        "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100\n",
        "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
        "    # end of batch loop...\n",
        "\n",
        "    # now that we've trained through the batches, get their average training accuracy\n",
        "    trainAcc.append( np.mean(batchAcc) )\n",
        "\n",
        "    # and get average losses across the batches\n",
        "    losses[epochi] = np.mean(batchLoss)\n",
        "\n",
        "    # test accuracy\n",
        "    X,y = next(iter(test_data)) # extract X,y from test dataloader\n",
        "    with torch.no_grad(): # deactivates autograd\n",
        "      yHat = net(X)\n",
        "\n",
        "    # compare the following really long line of code to the training accuracy lines\n",
        "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
        "  # end epochs\n",
        "\n",
        "  # function output\n",
        "  return trainAcc,testAcc,losses,net\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of9E8ClxMNsD"
      },
      "source": [
        "### Test the model once with a bit of data, just to make sure the code works.\n",
        "\n",
        "# generate the data\n",
        "theData = createSomeData(200)\n",
        "train_data = theData['train_data']\n",
        "test_data  = theData['test_data']\n",
        "\n",
        "# run the model\n",
        "trainAcc,testAcc,losses,net = function2trainTheModel(80,1)\n",
        "\n",
        "\n",
        "\n",
        "# show the results!\n",
        "fig,ax = plt.subplots(1,2,figsize=(13,4))\n",
        "\n",
        "ax[0].plot(losses.detach())\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_xlabel('epoch')\n",
        "ax[0].set_title('Losses')\n",
        "\n",
        "ax[1].plot(trainAcc,label='Train')\n",
        "ax[1].plot(testAcc,label='Test')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_title('Accuracy')\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmX6K49WMNuy"
      },
      "source": [
        "# Now for the experiment!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "196UnpfLYldk"
      },
      "source": [
        "# before the experiment, configure and confirm the metaparameters\n",
        "\n",
        "# specify the parameters for the model\n",
        "nNodesInModel = 80\n",
        "layersRange   = [ 1,5,10,20 ]\n",
        "nDatapoints   = np.arange(50,551,50)\n",
        "\n",
        "# create a legend for later plotting\n",
        "legend = []\n",
        "\n",
        "# print out the model architectures\n",
        "for lidx,layers in enumerate(layersRange):\n",
        "\n",
        "  # create a model\n",
        "  unitsperlayer = int(nNodesInModel/layersRange[lidx])\n",
        "  net = createTheQwertyNet(unitsperlayer,layers)[0]\n",
        "\n",
        "  # count its parameters (see lecture ANNs:Depth vs. breadth)\n",
        "  nparams = np.sum([ p.numel() for p in net.parameters() if p.requires_grad ])\n",
        "\n",
        "  legend.append( '%s layers, %s units, %s params' %(layers,unitsperlayer,nparams) )\n",
        "  print('This model will have %s layers, each with %s units, totalling %s parameters' %(layers,unitsperlayer,nparams))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J0jeXG3SnZZ"
      },
      "source": [
        "# note: takes ~5 mins\n",
        "\n",
        "# initialize results matrix\n",
        "results = np.zeros((len(nDatapoints),len(layersRange),2))\n",
        "\n",
        "for didx,pnts in enumerate(nDatapoints):\n",
        "\n",
        "  # create data (note: same data for each layer manipulation!)\n",
        "  theData = createSomeData(pnts)\n",
        "  train_data = theData['train_data']\n",
        "  test_data  = theData['test_data']\n",
        "\n",
        "\n",
        "  # now loop over layers\n",
        "  for lidx,layers in enumerate(layersRange):\n",
        "\n",
        "    unitsperlayer = int(nNodesInModel/layersRange[lidx])\n",
        "    trainAcc,testAcc,losses,net = function2trainTheModel(unitsperlayer,layers)\n",
        "\n",
        "    # average of last 5 accuracies and losses\n",
        "    results[didx,lidx,0] = np.mean( testAcc[-5:] )\n",
        "    results[didx,lidx,1] = torch.mean(losses[-5:]).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEegaXDsSnca"
      },
      "source": [
        "# show the results!\n",
        "\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
        "ax[0].plot(nDatapoints,results[:,:,1],'s-')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_xlabel('Number of data points')\n",
        "ax[0].legend(legend)\n",
        "ax[0].set_title('Losses')\n",
        "\n",
        "ax[1].plot(nDatapoints,results[:,:,0],'o-')\n",
        "ax[1].set_ylabel('Accuracy (%)')\n",
        "ax[1].set_xlabel('Number of data points')\n",
        "ax[1].set_title('Accuracy')\n",
        "ax[1].legend(legend)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrzrWK39SniK"
      },
      "source": [
        "# Interpretation:\n",
        "#   Learning depends more on the architecture and the nature of the problem, than on the number of parameters."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fExGSRvqvnAl"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLO0SxB-Snkq"
      },
      "source": [
        "# Additional explorations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOJhMXXISQ0J"
      },
      "source": [
        "# 1) The model learns faster and better with the Adam optimizer. In fact, I intentionally used SGD here to make the\n",
        "#    model worse for this demonstration! Change the optimizer to Adam. What do you think is a good learning rate?\n",
        "#    More importantly: Do the conclusions of this experiment hold for the Adam optimizer?\n",
        "#\n",
        "# 2) Add a timer to the experiment loop. Does the training duration relate to the number of layers or the number\n",
        "#    of parameters?\n",
        "#\n",
        "# 3) Do the two deepest models eventually learn if you increase the number of training epochs? (Note: because this\n",
        "#    question is only about the deepest models and because training time will increase, you need only test the two\n",
        "#    models, not all four.)\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}