{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1615925514977
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: ANNs\n",
    "### LECTURE: Comparing the number of hidden units\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia1C-oGK4uk1"
   },
   "source": [
    "# Import and organize the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MU7rvmWuhjud"
   },
   "source": [
    "# import dataset\n",
    "import pandas as pd\n",
    "iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "print( iris.head() )\n",
    "\n",
    "# some plots to show the data\n",
    "sns.pairplot(iris, hue='species')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vJPkH6Bfh01_"
   },
   "source": [
    "# organize the data\n",
    "\n",
    "# convert from pandas dataframe to tensor\n",
    "data = torch.tensor( iris[iris.columns[0:4]].values ).float()\n",
    "\n",
    "# transform species to number\n",
    "labels = torch.zeros(len(data), dtype=torch.long)\n",
    "# labels[iris.species=='setosa'] = 0 # don't need!\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica'] = 2\n",
    "\n",
    "labels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CFW7ywE4w_u"
   },
   "source": [
    "# Functions to create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v0JMIGb1iV_9"
   },
   "source": [
    "# Note the input into the function!\n",
    "def createIrisModel(nHidden):\n",
    "\n",
    "  # model architecture (with number of units soft-coded!)\n",
    "  ANNiris = nn.Sequential(\n",
    "      nn.Linear(4,nHidden),      # input layer\n",
    "      nn.ReLU(),                 # activation unit\n",
    "      nn.Linear(nHidden,nHidden),# hidden layer\n",
    "      nn.ReLU(),                 # activation unit\n",
    "      nn.Linear(nHidden,3),      # output unit\n",
    "      #nn.Softmax(dim=1),        # final activation unit (here for conceptual purposes, note the CEL function)\n",
    "        )\n",
    "\n",
    "  # loss function\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(ANNiris.parameters(),lr=.01)\n",
    "\n",
    "  return ANNiris,lossfun,optimizer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cVD1nFTli7TO"
   },
   "source": [
    "# a function to train the model\n",
    "\n",
    "def trainTheModel(ANNiris):\n",
    "\n",
    "  # initialize losses\n",
    "  losses = torch.zeros(numepochs)\n",
    "  ongoingAcc = []\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # forward pass\n",
    "    yHat = ANNiris(data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = lossfun(yHat,labels)\n",
    "    losses[epochi] = loss\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # final forward pass\n",
    "  predictions = ANNiris(data)\n",
    "\n",
    "  predlabels = torch.argmax(predictions,axis=1)\n",
    "  return 100*torch.mean((predlabels==labels).float())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm80pv3J48d1"
   },
   "source": [
    "# Run the experiment!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hWOzwwHTrdxz"
   },
   "source": [
    "numepochs  = 150\n",
    "numhiddens = np.arange(1,129)\n",
    "accuracies = []\n",
    "\n",
    "for nunits in numhiddens:\n",
    "\n",
    "  # create a fresh model instance\n",
    "  ANNiris,lossfun,optimizer = createIrisModel(nunits)\n",
    "\n",
    "  # run the model\n",
    "  acc = trainTheModel(ANNiris)\n",
    "  accuracies.append( acc )\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JYouZAY4i3jM"
   },
   "source": [
    "# report accuracy\n",
    "fig,ax = plt.subplots(1,figsize=(12,6))\n",
    "\n",
    "ax.plot(accuracies,'ko-',markerfacecolor='w',markersize=9)\n",
    "ax.plot(numhiddens[[0,-1]],[33,33],'--',color=[.8,.8,.8])\n",
    "ax.plot(numhiddens[[0,-1]],[67,67],'--',color=[.8,.8,.8])\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_xlabel('Number of hidden units')\n",
    "ax.set_title('Accuracy')\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zvLH4h6ek5Ax"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7MKj66sk5DY"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zIXi3fONIKVA"
   },
   "source": [
    "# 1) The results here show that models with fewer than ~50 hidden units have lackluster performance. Would these models\n",
    "#    eventually learn if they were given more training epochs? Try this by re-running the experiment using 500 epochs.\n",
    "#    Tip: Copy/paste the plotting code into a new cell to keep both plots. Or, take screenshots of the plots.\n",
    "#\n",
    "# 2) Going back to 150 epochs, explore the effect of changing the learning rate. This doesn't need to be a full parametric\n",
    "#    experiment; you can simply try is again using learning rates of .1, .01 (what we used in the video), and .001.\n",
    "#\n",
    "# 3) With simple models and small datasets, it's possible to test many different parameter settings. However, larger\n",
    "#    models take longer to train, and so running 128 tests is not always feasible. Modify the code to have the number of\n",
    "#    hidden units range from 1 to 128 in steps of 14. Plot the results on top of the results using steps of 1 (that is,\n",
    "#    show both results in the same graph). Does your interpretation change with fewer experiment runs?\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}