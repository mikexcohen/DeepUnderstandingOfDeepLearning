{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: ANNs\n",
    "### LECTURE: Learning rates comparison\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6919,
     "status": "ok",
     "timestamp": 1678596494434,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "j7-LiwqUMGYL"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4dat0StynDT"
   },
   "source": [
    "# Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "executionInfo": {
     "elapsed": 1015,
     "status": "ok",
     "timestamp": 1678596495445,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "j-SP8NPsMNRL",
    "outputId": "e045dd9a-98d1-400c-c4ea-ce889f47106e"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "nPerClust = 100\n",
    "blur = 1\n",
    "\n",
    "A = [  1, 1 ]\n",
    "B = [  5, 1 ]\n",
    "\n",
    "# generate data\n",
    "a = [ A[0]+np.random.randn(nPerClust)*blur , A[1]+np.random.randn(nPerClust)*blur ]\n",
    "b = [ B[0]+np.random.randn(nPerClust)*blur , B[1]+np.random.randn(nPerClust)*blur ]\n",
    "\n",
    "# true labels\n",
    "labels_np = np.vstack((np.zeros((nPerClust,1)),np.ones((nPerClust,1))))\n",
    "\n",
    "# concatanate into a matrix\n",
    "data_np = np.hstack((a,b)).T\n",
    "\n",
    "# convert to a pytorch tensor\n",
    "data = torch.tensor(data_np).float()\n",
    "labels = torch.tensor(labels_np).float()\n",
    "\n",
    "# show the data\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'bs')\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'ko')\n",
    "plt.title('The qwerties!')\n",
    "plt.xlabel('qwerty dimension 1')\n",
    "plt.ylabel('qwerty dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brU3Nrf8yqw6"
   },
   "source": [
    "# Functions to build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678596495446,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "krQeh5wYMNla"
   },
   "outputs": [],
   "source": [
    "def createANNmodel(learningRate):\n",
    "\n",
    "  # model architecture\n",
    "  ANNclassify = nn.Sequential(\n",
    "      nn.Linear(2,1),   # input layer\n",
    "      nn.ReLU(),        # activation unit\n",
    "      nn.Linear(1,1),   # output unit\n",
    "      #nn.Sigmoid(),    # final activation unit (not needed b/c we use BCEWithLogitsLoss)\n",
    "        )\n",
    "\n",
    "  # loss function\n",
    "  lossfun = nn.BCEWithLogitsLoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(ANNclassify.parameters(),lr=learningRate)\n",
    "\n",
    "  # model output\n",
    "  return ANNclassify,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1678596495446,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "of9E8ClxMNsD"
   },
   "outputs": [],
   "source": [
    "# a function that trains the model\n",
    "\n",
    "# a fixed parameter\n",
    "numepochs = 1000\n",
    "\n",
    "def trainTheModel(ANNmodel):\n",
    "\n",
    "  # initialize losses\n",
    "  losses = torch.zeros(numepochs)\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # forward pass\n",
    "    yHat = ANNmodel(data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = lossfun(yHat,labels)\n",
    "    losses[epochi] = loss\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "  \n",
    "  \n",
    "  # final forward pass\n",
    "  predictions = ANNmodel(data)\n",
    "    \n",
    "  # compute the predictions and report accuracy\n",
    "  # NOTE: shouldn't it be predictions>.5??\n",
    "  totalacc = 100*torch.mean(((predictions>0) == labels).float())\n",
    "  \n",
    "  return losses,predictions,totalacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgRDKuoDy14D"
   },
   "source": [
    "# Test the new code by running it once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "executionInfo": {
     "elapsed": 2641,
     "status": "ok",
     "timestamp": 1678596498084,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "SatsWGs3x6Kg",
    "outputId": "b15c359a-37b1-40f5-a675-7ad7f8ea6afc"
   },
   "outputs": [],
   "source": [
    "# create everything\n",
    "ANNclassify,lossfun,optimizer = createANNmodel(.01)\n",
    "\n",
    "# run it\n",
    "losses,predictions,totalacc = trainTheModel(ANNclassify)\n",
    "\n",
    "# report accuracy\n",
    "print('Final accuracy: %g%%' %totalacc)\n",
    "\n",
    "\n",
    "# show the losses\n",
    "plt.plot(losses.detach(),'o',markerfacecolor='w',linewidth=.1)\n",
    "plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cgS0-1vy4sZ"
   },
   "source": [
    "# Now for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42057,
     "status": "ok",
     "timestamp": 1678596540137,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "i1TCt0mpMNxC"
   },
   "outputs": [],
   "source": [
    "# the set of learning rates to test\n",
    "learningrates = np.linspace(.001,.1,40)\n",
    "\n",
    "# initialize results output\n",
    "accByLR = []\n",
    "allLosses = np.zeros((len(learningrates),numepochs))\n",
    "\n",
    "\n",
    "# loop through learning rates\n",
    "for i,lr in enumerate(learningrates):\n",
    "  \n",
    "  # create and run the model\n",
    "  ANNclassify,lossfun,optimizer = createANNmodel(lr)\n",
    "  losses,predictions,totalacc = trainTheModel(ANNclassify)\n",
    "\n",
    "  # store the results\n",
    "  accByLR.append(totalacc)\n",
    "  allLosses[i,:] = losses.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 477,
     "status": "ok",
     "timestamp": 1678596540607,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "1EpSpm3klet-",
    "outputId": "0f41010c-481a-423a-891c-38a415130907"
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "ax[0].plot(learningrates,accByLR,'s-')\n",
    "ax[0].set_xlabel('Learning rate')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_title('Accuracy by learning rate')\n",
    "\n",
    "ax[1].plot(allLosses.T)\n",
    "ax[1].set_title('Losses by learning rate')\n",
    "ax[1].set_xlabel('Epoch number')\n",
    "ax[1].set_ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1678596540608,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "x_lVYxTsi871",
    "outputId": "22dbf298-f269-495e-b919-7917966bcf29"
   },
   "outputs": [],
   "source": [
    "# proportion of runs where the model had at least 70% accuracy\n",
    "sum(torch.tensor(accByLR)>70)/len(accByLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8hKZlLlzFgB"
   },
   "source": [
    "# Run a meta-experiment to get more reliable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 539332,
     "status": "ok",
     "timestamp": 1678597079935,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "kwaa_O67EU6t",
    "outputId": "48b3eb00-d23a-4f30-e6fa-4f2071dd94db"
   },
   "outputs": [],
   "source": [
    "# run a \"meta-experiment\" by repeating the experiment N times\n",
    "#  (different random weight initializations each time)\n",
    "# note: this cell takes ~7 mins.\n",
    "\n",
    "# number of times to iterate through the experiment\n",
    "numExps = 50\n",
    "\n",
    "# matrix to store all results\n",
    "accMeta = np.zeros((numExps,len(learningrates)))\n",
    "\n",
    "# fewer epochs to reduce computation time\n",
    "numepochs = 500\n",
    "\n",
    "# now for the experiment\n",
    "for expi in range(numExps):\n",
    "  for i,lr in enumerate(learningrates):\n",
    "    \n",
    "    # create and run the model\n",
    "    ANNclassify,lossfun,optimizer = createANNmodel(lr)\n",
    "    losses,predictions,totalacc = trainTheModel(ANNclassify)\n",
    "\n",
    "    # store the results\n",
    "    accMeta[expi,i] = totalacc\n",
    "\n",
    "\n",
    "\n",
    "# now plot the results, averaged over experiments\n",
    "plt.plot(learningrates,np.mean(accMeta,axis=0),'s-')\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy by learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1678597079936,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "x15VUzbJdeg_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8kSb8b-XIJh"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1678597079937,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -120
    },
    "id": "Hjtq7gUAXIbT"
   },
   "outputs": [],
   "source": [
    "# 1) The closeness of the qwerties groups is determined by the XY locations of the centroids, and by the blur parameter.\n",
    "#    Try increasing or decreasing the blur (e.g., to 2 or .5). How does this affect the number of times that the model\n",
    "#    successfully learned to categorize the two conditions?\n",
    "# \n",
    "# 2) The mean of a set of numbers is easily interpretable only if the data are roughly normally or uniformly distributed \n",
    "#    (see lecture \"Mean and variance\" in Math section). Do you think the mean is a valid description of the performance\n",
    "#    of the model's accuracy in the meta-experiment? Use a different metric (e.g., one we discussed in this video!) and \n",
    "#    plot that result on the same graph as the average. You might need to do some normalization to get them in the same \n",
    "#    range. Does this alternative method lead to a different conclusion?\n",
    "# \n",
    "# 3) Related to the previous comment, perhaps showing an image of the performance (variable accMeta) would be more \n",
    "#    appropriate. Create a heat map that shows learning rate on the x-axis, experiment repetitions on the y-axis, and\n",
    "#    the final accuracy in color. Label the axes and specify suitable color boundaries.\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNBYQQAr1ocnC0YgpAy2daC",
   "provenance": [
    {
     "file_id": "10_geQnah5AvMsm8VDAQwNPhypOXradar",
     "timestamp": 1615893340208
    },
    {
     "file_id": "1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD",
     "timestamp": 1615877547147
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
