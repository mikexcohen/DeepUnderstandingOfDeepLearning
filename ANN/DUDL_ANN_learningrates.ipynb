{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DUDL_ANN_learningrates.ipynb","provenance":[{"file_id":"10_geQnah5AvMsm8VDAQwNPhypOXradar","timestamp":1615893340208},{"file_id":"1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD","timestamp":1615877547147}],"collapsed_sections":[],"authorship_tag":"ABX9TyNBYQQAr1ocnC0YgpAy2daC"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bhWV8oes-wKR"},"source":["# COURSE: A deep understanding of deep learning\n","## SECTION: ANNs\n","### LECTURE: Learning rates comparison\n","#### TEACHER: Mike X Cohen, sincxpress.com\n","##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202302"]},{"cell_type":"code","metadata":{"id":"j7-LiwqUMGYL"},"source":["# import libraries\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","from IPython import display\n","display.set_matplotlib_formats('svg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C4dat0StynDT"},"source":["# Create the data"]},{"cell_type":"code","metadata":{"id":"j-SP8NPsMNRL"},"source":["# parameters\n","nPerClust = 100\n","blur = 1\n","\n","A = [  1, 1 ]\n","B = [  5, 1 ]\n","\n","# generate data\n","a = [ A[0]+np.random.randn(nPerClust)*blur , A[1]+np.random.randn(nPerClust)*blur ]\n","b = [ B[0]+np.random.randn(nPerClust)*blur , B[1]+np.random.randn(nPerClust)*blur ]\n","\n","# true labels\n","labels_np = np.vstack((np.zeros((nPerClust,1)),np.ones((nPerClust,1))))\n","\n","# concatanate into a matrix\n","data_np = np.hstack((a,b)).T\n","\n","# convert to a pytorch tensor\n","data = torch.tensor(data_np).float()\n","labels = torch.tensor(labels_np).float()\n","\n","# show the data\n","fig = plt.figure(figsize=(5,5))\n","plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'bs')\n","plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'ko')\n","plt.title('The qwerties!')\n","plt.xlabel('qwerty dimension 1')\n","plt.ylabel('qwerty dimension 2')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"brU3Nrf8yqw6"},"source":["# Functions to build and train the model"]},{"cell_type":"code","metadata":{"id":"krQeh5wYMNla"},"source":["def createANNmodel(learningRate):\n","\n","  # model architecture\n","  ANNclassify = nn.Sequential(\n","      nn.Linear(2,1),   # input layer\n","      nn.ReLU(),        # activation unit\n","      nn.Linear(1,1),   # output unit\n","      #nn.Sigmoid(),    # final activation unit (not needed b/c we use BCEWithLogitsLoss)\n","        )\n","\n","  # loss function\n","  lossfun = nn.BCEWithLogitsLoss()\n","\n","  # optimizer\n","  optimizer = torch.optim.SGD(ANNclassify.parameters(),lr=learningRate)\n","\n","  # model output\n","  return ANNclassify,lossfun,optimizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"of9E8ClxMNsD"},"source":["# a function that trains the model\n","\n","# a fixed parameter\n","numepochs = 1000\n","\n","def trainTheModel(ANNmodel):\n","\n","  # initialize losses\n","  losses = torch.zeros(numepochs)\n","\n","  # loop over epochs\n","  for epochi in range(numepochs):\n","\n","    # forward pass\n","    yHat = ANNmodel(data)\n","\n","    # compute loss\n","    loss = lossfun(yHat,labels)\n","    losses[epochi] = loss\n","\n","    # backprop\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","  \n","  \n","  \n","  # final forward pass\n","  predictions = ANNmodel(data)\n","    \n","  # compute the predictions and report accuracy\n","  # NOTE: shouldn't it be predictions>.5??\n","  totalacc = 100*torch.mean(((predictions>0) == labels).float())\n","  \n","  return losses,predictions,totalacc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XgRDKuoDy14D"},"source":["# Test the new code by running it once"]},{"cell_type":"code","metadata":{"id":"SatsWGs3x6Kg"},"source":["# create everything\n","ANNclassify,lossfun,optimizer = createANNmodel(.01)\n","\n","# run it\n","losses,predictions,totalacc = trainTheModel(ANNclassify)\n","\n","# report accuracy\n","print('Final accuracy: %g%%' %totalacc)\n","\n","\n","# show the losses\n","plt.plot(losses.detach(),'o',markerfacecolor='w',linewidth=.1)\n","plt.xlabel('Epoch'), plt.ylabel('Loss')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3cgS0-1vy4sZ"},"source":["# Now for the experiment"]},{"cell_type":"code","metadata":{"id":"i1TCt0mpMNxC"},"source":["# the set of learning rates to test\n","learningrates = np.linspace(.001,.1,40)\n","\n","# initialize results output\n","accByLR = []\n","allLosses = np.zeros((len(learningrates),numepochs))\n","\n","\n","# loop through learning rates\n","for i,lr in enumerate(learningrates):\n","  \n","  # create and run the model\n","  ANNclassify,lossfun,optimizer = createANNmodel(lr)\n","  losses,predictions,totalacc = trainTheModel(ANNclassify)\n","\n","  # store the results\n","  accByLR.append(totalacc)\n","  allLosses[i,:] = losses.detach()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1EpSpm3klet-"},"source":["# plot the results\n","fig,ax = plt.subplots(1,2,figsize=(12,4))\n","\n","ax[0].plot(learningrates,accByLR,'s-')\n","ax[0].set_xlabel('Learning rate')\n","ax[0].set_ylabel('Accuracy')\n","ax[0].set_title('Accuracy by learning rate')\n","\n","ax[1].plot(allLosses.T)\n","ax[1].set_title('Losses by learning rate')\n","ax[1].set_xlabel('Epoch number')\n","ax[1].set_ylabel('Loss')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_lVYxTsi871"},"source":["# proportion of runs where the model had at least 70% accuracy\n","sum(torch.tensor(accByLR)>70)/len(accByLR)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K8hKZlLlzFgB"},"source":["# Run a meta-experiment to get more reliable results"]},{"cell_type":"code","metadata":{"id":"kwaa_O67EU6t"},"source":["# run a \"meta-experiment\" by repeating the experiment N times\n","#  (different random weight initializations each time)\n","# note: this cell takes ~7 mins.\n","\n","# number of times to iterate through the experiment\n","numExps = 50\n","\n","# matrix to store all results\n","accMeta = np.zeros((numExps,len(learningrates)))\n","\n","# fewer epochs to reduce computation time\n","numepochs = 500\n","\n","# now for the experiment\n","for expi in range(numExps):\n","  for i,lr in enumerate(learningrates):\n","    \n","    # create and run the model\n","    ANNclassify,lossfun,optimizer = createANNmodel(lr)\n","    losses,predictions,totalacc = trainTheModel(ANNclassify)\n","\n","    # store the results\n","    accMeta[expi,i] = totalacc\n","\n","\n","\n","# now plot the results, averaged over experiments\n","plt.plot(learningrates,np.mean(accMeta,axis=0),'s-')\n","plt.xlabel('Learning rate')\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy by learning rate')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x15VUzbJdeg_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j8kSb8b-XIJh"},"source":["# Additional explorations"]},{"cell_type":"code","metadata":{"id":"Hjtq7gUAXIbT"},"source":["# 1) The closeness of the qwerties groups is determined by the XY locations of the centroids, and by the blur parameter.\n","#    Try increasing or decreasing the blur (e.g., to 2 or .5). How does this affect the number of times that the model\n","#    successfully learned to categorize the two conditions?\n","# \n","# 2) The mean of a set of numbers is easily interpretable only if the data are roughly normally or uniformly distributed \n","#    (see lecture \"Mean and variance\" in Math section). Do you think the mean is a valid description of the performance\n","#    of the model's accuracy in the meta-experiment? Use a different metric (e.g., one we discussed in this video!) and \n","#    plot that result on the same graph as the average. You might need to do some normalization to get them in the same \n","#    range. Does this alternative method lead to a different conclusion?\n","# \n","# 3) Related to the previous comment, perhaps showing an image of the performance (variable accMeta) would be more \n","#    appropriate. Create a heat map that shows learning rate on the x-axis, experiment repetitions on the y-axis, and\n","#    the final accuracy in color. Label the axes and specify suitable color boundaries.\n","# "],"execution_count":null,"outputs":[]}]}