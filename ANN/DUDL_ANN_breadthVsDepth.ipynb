{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DUDL_ANN_breadthVsDepth.ipynb","provenance":[{"file_id":"1V0mkFVCMTAd0iq30FIpXspezzE7PpVrX","timestamp":1616523401452},{"file_id":"1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK","timestamp":1615925514977}],"collapsed_sections":[],"authorship_tag":"ABX9TyN4056RCJOTpz/xZ+H60BLF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bhWV8oes-wKR"},"source":["# COURSE: A deep understanding of deep learning\n","## SECTION: ANNs\n","### LECTURE: Model depth vs. breadth\n","#### TEACHER: Mike X Cohen, sincxpress.com\n","##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202302"]},{"cell_type":"code","metadata":{"id":"YeuAheYyhdZw"},"source":["# import libraries\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython import display\n","display.set_matplotlib_formats('svg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ViJutqaaNb2"},"source":["# Import and organize the data"]},{"cell_type":"code","metadata":{"id":"MU7rvmWuhjud"},"source":["# import dataset (comes with seaborn)\n","import seaborn as sns\n","iris = sns.load_dataset('iris')\n","\n","# convert from pandas dataframe to tensor\n","data = torch.tensor( iris[iris.columns[0:4]].values ).float()\n","\n","# transform species to number\n","labels = torch.zeros(len(data), dtype=torch.long)\n","# labels[iris.species=='setosa'] = 0 # don't need!\n","labels[iris.species=='versicolor'] = 1\n","labels[iris.species=='virginica'] = 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jCuMSE6baRar"},"source":["# Construct and sanity-check the model"]},{"cell_type":"code","metadata":{"id":"eZMzMLxfULjf"},"source":["# create a class for the model\n","\n","class ANNiris(nn.Module):\n","  def __init__(self,nUnits,nLayers):\n","    super().__init__()\n","\n","    # create dictionary to store the layers\n","    self.layers = nn.ModuleDict()\n","    self.nLayers = nUnits#nLayers#\n","\n","    ### input layer\n","    self.layers['input'] = nn.Linear(4,nUnits)\n","    \n","    ### hidden layers\n","    for i in range(nLayers):\n","      self.layers[f'hidden{i}'] = nn.Linear(nUnits,nUnits)\n","\n","    ### output layer\n","    self.layers['output'] = nn.Linear(nUnits,3)\n","  \n","\n","  # forward pass\n","  def forward(self,x):\n","    # input layer (note: the code in the video omits the relu after this layer)\n","    x = F.relu( self.layers['input'](x) )\n","\n","    # hidden layers\n","    for i in range(self.nLayers):\n","      x = F.relu( self.layers[f'hidden{i}'](x) )\n","    \n","    # return output layer\n","    x = self.layers['output'](x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-GmvNgEYgHK"},"source":["# generate an instance of the model and inspect it\n","nUnitsPerLayer = 12\n","nLayers = 4\n","net = ANNiris(nUnitsPerLayer,nLayers)\n","net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwtrXLSNYyC8"},"source":["# A quick test of running some numbers through the model. \n","# This simply ensures that the architecture is internally consistent.\n","\n","\n","# 10 samples, 4 dimensions\n","tmpx = torch.randn(10,4)\n","\n","# run it through the DL\n","y = net(tmpx)\n","\n","# exam the shape of the output\n","print( y.shape ), print(' ')\n","\n","# and the output itself\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YL7cvyjUaXjc"},"source":["# Create a function that trains the model"]},{"cell_type":"code","metadata":{"id":"cVD1nFTli7TO"},"source":["# a function to train the model\n","\n","def trainTheModel(theModel):\n","  \n","  # define the loss function and optimizer\n","  lossfun = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.SGD(theModel.parameters(),lr=.01)\n","\n","  # loop over epochs\n","  for epochi in range(numepochs):\n","\n","    # forward pass\n","    yHat = theModel(data)\n","\n","    # compute loss\n","    loss = lossfun(yHat,labels)\n","\n","    # backprop\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","  \n","  \n","  # final forward pass to get accuracy\n","  predictions = theModel(data)\n","  predlabels = torch.argmax(predictions,axis=1)\n","  acc = 100*torch.mean((predlabels == labels).float())\n","\n","  # total number of trainable parameters in the model\n","  nParams = sum(p.numel() for p in theModel.parameters() if p.requires_grad)\n","\n","  # function outputs\n","  return acc,nParams"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"41R4X0MCaxVc"},"source":["# test the function once\n","\n","numepochs = 2500\n","acc = trainTheModel(net)\n","\n","# check the outputs \n","acc # tuple containing (accuracy,nparams)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lE1lhk5356g7"},"source":["# Now for the experiment!"]},{"cell_type":"code","metadata":{"id":"hWOzwwHTrdxz"},"source":["# this cell takes ~2 mins\n","\n","# define the model parameters\n","numlayers = range(1,6)         # number of hidden layers\n","numunits  = np.arange(4,101,3) # units per hidden layer\n","\n","# initialize output matrices\n","accuracies  = np.zeros((len(numunits),len(numlayers)))\n","totalparams = np.zeros((len(numunits),len(numlayers)))\n","\n","# number of training epochs\n","numepochs = 500\n","\n","\n","# start the experiment!\n","for unitidx in range(len(numunits)):\n","  for layeridx in range(len(numlayers)):\n","\n","    # create a fresh model instance\n","    net = ANNiris(numunits[unitidx],numlayers[layeridx])\n","\n","    # run the model and store the results\n","    acc,nParams = trainTheModel(net)\n","    accuracies[unitidx,layeridx] = acc\n","\n","    # store the total number of parameters in the model\n","    totalparams[unitidx,layeridx] = nParams\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYouZAY4i3jM"},"source":["# show accuracy as a function of model depth\n","fig,ax = plt.subplots(1,figsize=(12,6))\n","\n","ax.plot(numunits,accuracies,'o-',markerfacecolor='w',markersize=9)\n","ax.plot(numunits[[0,-1]],[33,33],'--',color=[.8,.8,.8])\n","ax.plot(numunits[[0,-1]],[67,67],'--',color=[.8,.8,.8])\n","ax.legend(numlayers)\n","ax.set_ylabel('accuracy')\n","ax.set_xlabel('Number of hidden units')\n","ax.set_title('Accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"St6NI4qBk4tO"},"source":["# Maybe it's simply a matter of more parameters -> better performance?\n","\n","# vectorize for convenience\n","x = totalparams.flatten()\n","y = accuracies.flatten()\n","\n","# correlation between them\n","r = np.corrcoef(x,y)[0,1]\n","\n","# scatter plot\n","plt.plot(x,y,'o')\n","plt.xlabel('Number of parameters')\n","plt.ylabel('Accuracy')\n","plt.title('Correlation: r=' + str(np.round(r,3)))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ix4I-SgJzXX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JmraVzTcJ0x1"},"source":["# Additional explorations"]},{"cell_type":"code","metadata":{"id":"pml6nCTcAMWC"},"source":["# 1) Try it again with 1000 training epochs. Do the deeper models eventually learn?\n","# \n","# 2) The categories are coded a \"0\", \"1\", and \"2\". Is there something special about those numbers?\n","#    Recode the labels to be, e.g., 5, 10, and 17. Or perhaps -2, 0, and 2. Is the model still able to learn?\n","# "],"execution_count":null,"outputs":[]}]}