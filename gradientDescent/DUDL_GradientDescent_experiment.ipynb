{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1qFigOOWXcSNyA6hPpZnF-QqhFEB1e-hy",
     "timestamp": 1597210805829
    },
    {
     "file_id": "1kGRo0g3UXxXpJuQSEtpKjEGA1Vxbaz8S",
     "timestamp": 1597128018290
    },
    {
     "file_id": "1U4oG0A3DFC-XBWhvecYeA3YYReqHpShX",
     "timestamp": 1594575042741
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Gradient descent\n",
    "### LECTURE: Parametric experiments on g.d.\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JL_0UKJOj1YP"
   },
   "source": [
    "# import all necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-MNFF7SbkMC"
   },
   "source": [
    "# Running experiments to understand gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tKi89Y2dqvPl"
   },
   "source": [
    "# the function\n",
    "x  = np.linspace(-2*np.pi,2*np.pi,401)\n",
    "fx = np.sin(x) * np.exp(-x**2*.05)\n",
    "\n",
    "# and its derivative\n",
    "df = np.cos(x)*np.exp(-x**2*.05) + np.sin(x)*(-.1*x)*np.exp(-x**2*.05) \n",
    "\n",
    "# quick plot for inspection\n",
    "plt.plot(x,fx, x,df)\n",
    "plt.legend(['f(x)','df']);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wnpnbf7lnc1n"
   },
   "source": [
    "# function (note: over-writing variable names!)\n",
    "def fx(x):\n",
    "  return np.sin(x) * np.exp(-x**2*.05)\n",
    "\n",
    "# derivative function\n",
    "def deriv(x):\n",
    "  return np.cos(x)*np.exp(-x**2*.05) - np.sin(x)*.1*x*np.exp(-x**2*.05)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q6zswvtJk4_x"
   },
   "source": [
    "# random starting point\n",
    "localmin = np.random.choice(x,1)#np.array([6])#\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = .01\n",
    "training_epochs = 1000\n",
    "\n",
    "# run through training\n",
    "for i in range(training_epochs):\n",
    "  grad = deriv(localmin)\n",
    "  localmin = localmin - learning_rate*grad\n",
    "\n",
    "\n",
    "\n",
    "# plot the results\n",
    "plt.plot(x,fx(x), x,deriv(x),'--')\n",
    "plt.plot(localmin,deriv(localmin),'ro')\n",
    "plt.plot(localmin,fx(localmin),'ro')\n",
    "\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(['f(x)','df','f(x) min'])\n",
    "plt.title('Empirical local minimum: %s'%localmin[0])\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR3P03O93vrz"
   },
   "source": [
    "# Run parametric experiments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ybBbe3ARnjf2"
   },
   "source": [
    "# Experiment 1: systematically varying the starting locations\n",
    "\n",
    "startlocs = np.linspace(-5,5,50)\n",
    "finalres = np.zeros(len(startlocs))\n",
    "\n",
    "# loop over starting points\n",
    "for idx,localmin in enumerate(startlocs):\n",
    "  \n",
    "  # run through training\n",
    "  for i in range(training_epochs):\n",
    "    grad = deriv(localmin)\n",
    "    localmin = localmin - learning_rate*grad\n",
    "  \n",
    "  # store the final guess\n",
    "  finalres[idx] = localmin\n",
    "\n",
    "\n",
    "# plot the results\n",
    "plt.plot(startlocs,finalres,'s-')\n",
    "plt.xlabel('Starting guess')\n",
    "plt.ylabel('Final guess')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8Fef1yiqnjuE"
   },
   "source": [
    "# Experiment 2: systematically varying the learning rate\n",
    "\n",
    "learningrates = np.linspace(1e-10,1e-1,50)\n",
    "finalres = np.zeros(len(learningrates))\n",
    "\n",
    "# loop over learning rates\n",
    "for idx,learningRate in enumerate(learningrates):\n",
    "  \n",
    "  # force starting guess to 0\n",
    "  localmin = 0\n",
    "\n",
    "  # run through training\n",
    "  for i in range(training_epochs):\n",
    "    grad = deriv(localmin)\n",
    "    localmin = localmin - learningRate*grad\n",
    "  \n",
    "  # store the final guess\n",
    "  finalres[idx] = localmin\n",
    "\n",
    "\n",
    "plt.plot(learningrates,finalres,'s-')\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Final guess')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1d4brqbtvoPJ"
   },
   "source": [
    "# Experiment 3: interaction between learning rate and training epochs\n",
    "\n",
    "# setup parameters\n",
    "learningrates = np.linspace(1e-10,1e-1,50)\n",
    "training_epochs = np.round(np.linspace(10,500,40))\n",
    "\n",
    "# initialize matrix to store results\n",
    "finalres = np.zeros((len(learningrates),len(training_epochs)))\n",
    "\n",
    "\n",
    "\n",
    "# loop over learning rates\n",
    "for Lidx,learningRate in enumerate(learningrates):\n",
    "\n",
    "  # loop over training epochs\n",
    "  for Eidx,trainEpochs in enumerate(training_epochs):\n",
    "  \n",
    "    # run through training (again fixing starting location)\n",
    "    localmin = 0\n",
    "    for i in range(int(trainEpochs)):\n",
    "      grad = deriv(localmin)\n",
    "      localmin = localmin - learningRate*grad\n",
    "    \n",
    "    # store the final guess\n",
    "    finalres[Lidx,Eidx] = localmin\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0dL9IfScwZRd"
   },
   "source": [
    "# plot the results\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "plt.imshow(finalres,extent=[learningrates[0],learningrates[-1],training_epochs[0],training_epochs[-1]],\n",
    "           aspect='auto',origin='lower',vmin=-1.45,vmax=-1.2)\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Training epochs')\n",
    "plt.title('Final guess')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# another visualization\n",
    "plt.plot(learningrates,finalres)\n",
    "plt.xlabel('Learning rates')\n",
    "plt.ylabel('Final function estimate')\n",
    "plt.title('Each line is a training epochs N')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YXbts2Fkx-xT"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvGE12fEiui7"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m54Y_kYXiwO9"
   },
   "source": [
    "# 1) In experiment 3, set the starting location to be 1.6. Re-run the experiment and the image. You'll need to re-adjust \n",
    "#    the figure color limits; check the line plots at the top of the code to determine a useful color range. Does the new\n",
    "#    starting value change your conclusions about the interaction between learning rate and training epochs?\n",
    "# \n",
    "# 2) In the same experiment, now change the starting location to be random (use code: np.random.choice(x,1)). How do these\n",
    "#    results look? Are you surprised? Are the results of this experiment still interpretable and what does this tell you \n",
    "#    about running experiments in DL?\n",
    "# "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}