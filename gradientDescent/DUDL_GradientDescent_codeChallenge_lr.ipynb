{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ee89e4",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ydebessu/DeepUnderstandingOfDeepLearning/blob/main/gradientDescent/DUDL_GradientDescent_codeChallenge_lr.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Gradient descent\n",
    "### LECTURE: CodeChallenge: dynamic learning rates\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JL_0UKJOj1YP"
   },
   "outputs": [],
   "source": [
    "# import all necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeYMLgEvZY1X"
   },
   "source": [
    "# Create the function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwTBzVJsoKbg"
   },
   "outputs": [],
   "source": [
    "# define a range for x\n",
    "x = np.linspace(-2,2,2001)\n",
    "\n",
    "# function (as a function)\n",
    "def fx(x):\n",
    "  return 3*x**2 - 3*x + 4\n",
    "\n",
    "# derivative function\n",
    "def deriv(x):\n",
    "  return 6*x - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWC2T55ovJ-4"
   },
   "source": [
    "### G.D. using a fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB8dmH-nvJ_D"
   },
   "outputs": [],
   "source": [
    "# random starting point\n",
    "localmin = np.random.choice(x,1)\n",
    "initval = localmin[:] # store the initial value\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = .01\n",
    "training_epochs = 50\n",
    "\n",
    "# run through training and store all the results\n",
    "modelparamsFixed = np.zeros((training_epochs,3))\n",
    "for i in range(training_epochs):\n",
    "\n",
    "  # compute gradient\n",
    "  grad = deriv(localmin)\n",
    "\n",
    "  # non-adaptive learning rate\n",
    "  lr = learning_rate\n",
    "\n",
    "  # update parameter according to g.d.\n",
    "  localmin = localmin - lr*grad\n",
    "\n",
    "  # store the parameters\n",
    "  modelparamsFixed[i,0] = localmin[0]\n",
    "  modelparamsFixed[i,1] = grad[0]\n",
    "  modelparamsFixed[i,2] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G24R3zyh6XWA"
   },
   "source": [
    "### G.D. using a gradient-based learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M22aVI6xVIbk"
   },
   "outputs": [],
   "source": [
    "# random starting point\n",
    "localmin = np.random.choice(x,1)\n",
    "initval = localmin[:] # store the initial value\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = .01\n",
    "\n",
    "# run through training and store all the results\n",
    "modelparamsGrad = np.zeros((training_epochs,3))\n",
    "for i in range(training_epochs):\n",
    "\n",
    "  # compute gradient\n",
    "  grad = deriv(localmin)\n",
    "\n",
    "  # adapt the learning rate according to the gradient\n",
    "  lr = learning_rate*np.abs(grad)\n",
    "\n",
    "  # update parameter according to g.d.\n",
    "  localmin = localmin - lr*grad\n",
    "\n",
    "  # store the parameters\n",
    "  modelparamsGrad[i,0] = localmin[0]\n",
    "  modelparamsGrad[i,1] = grad[0]\n",
    "  modelparamsGrad[i,2] = lr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49VT4I5B6rhQ"
   },
   "source": [
    "### G.D. using a time-based learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6OWQULR30oV"
   },
   "outputs": [],
   "source": [
    "# redefine parameters\n",
    "learning_rate = .1\n",
    "localmin = initval\n",
    "\n",
    "# run through training and store all the results\n",
    "modelparamsTime = np.zeros((training_epochs,3))\n",
    "for i in range(training_epochs):\n",
    "  grad = deriv(localmin)\n",
    "  lr = learning_rate*(1-(i+1)/training_epochs)\n",
    "  localmin = localmin - lr*grad\n",
    "  modelparamsTime[i,0] = localmin[0]\n",
    "  modelparamsTime[i,1] = grad[0]\n",
    "  modelparamsTime[i,2] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEYWC-m36yYX"
   },
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AInqnFtkVIeb"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(10,3))\n",
    "\n",
    "# generate the plots\n",
    "for i in range(3):\n",
    "  ax[i].plot(modelparamsFixed[:,i],'o-',markerfacecolor='w')\n",
    "  ax[i].plot(modelparamsGrad[:,i],'o-',markerfacecolor='w')\n",
    "  ax[i].plot(modelparamsTime[:,i],'o-',markerfacecolor='w')\n",
    "  ax[i].set_xlabel('Iteration')\n",
    "\n",
    "ax[0].set_ylabel('Local minimum')\n",
    "ax[1].set_ylabel('Derivative')\n",
    "ax[2].set_ylabel('Learning rate')\n",
    "ax[2].legend(['Fixed l.r.','Grad-based l.r.','Time-based l.r.'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvGE12fEiui7"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m54Y_kYXiwO9"
   },
   "outputs": [],
   "source": [
    "# 1) Change the initial learning rate in the \"time\" experiment from .1 to .01. Do you still reach the same conclusion that\n",
    "#    dynamic learning rates are better than a fixed learning rate?\n",
    "#\n",
    "# 2) Compute the average of all time-based learning rates (see variable 'modelparamsTime'). Next, replace the fixed\n",
    "#    learning rate with the average over all dynamic learning rates. How does that affect the model's performance?\n",
    "#\n",
    "# 3) Going back to the original code (without the modifications above), you saw that the fixed learning rate model didn't\n",
    "#    get to the correct local minimum. What happens if you increase the number of training epochs from 50 to 500? Does that\n",
    "#    improve the situation, and what does that tell you about the relationship between learning rate and training epochs?\n",
    "#\n",
    "# 4) The code here initializes the starting value as a random number, which will differ for each learning rate method.\n",
    "#    Is that appropriate or inappropriate for this experiment? Why? Change the code so that the starting value is the\n",
    "#    same for all three learning rate models.\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
