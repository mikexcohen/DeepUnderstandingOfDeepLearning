{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DUDL_GradientDescent_CodeChallengeStartValue.ipynb",
   "provenance": [
    {
     "file_id": "1AKaKETDfNqVEFzv5BnYGSjHfaPtFpH4x",
     "timestamp": 1617033326873
    },
    {
     "file_id": "1qFigOOWXcSNyA6hPpZnF-QqhFEB1e-hy",
     "timestamp": 1597210805829
    },
    {
     "file_id": "1kGRo0g3UXxXpJuQSEtpKjEGA1Vxbaz8S",
     "timestamp": 1597128018290
    },
    {
     "file_id": "1U4oG0A3DFC-XBWhvecYeA3YYReqHpShX",
     "timestamp": 1594575042741
    }
   ],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyNVWu+09BvHo8TdA+4YYlwi"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Gradient descent\n",
    "### LECTURE: CodeChallenge: Unfortunate starting value\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JL_0UKJOj1YP"
   },
   "source": [
    "# import all necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeYMLgEvZY1X"
   },
   "source": [
    "# Gradient descent in 1D"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YwTBzVJsoKbg"
   },
   "source": [
    "# function (as a function)\n",
    "def fx(x):\n",
    "  return np.cos(2*np.pi*x) + x**2\n",
    "\n",
    "# derivative function\n",
    "def deriv(x):\n",
    "  return -2*np.pi*np.sin(2*np.pi*x) + 2*x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_qRE_fHqUL6n"
   },
   "source": [
    "# plot the function and its derivative\n",
    "\n",
    "# define a range for x\n",
    "x = np.linspace(-2,2,2001)\n",
    "\n",
    "# plotting\n",
    "plt.plot(x,fx(x), x,deriv(x))\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(['y','dy'])\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r-f27UUkZYpG"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-nzWuHfWVHyU"
   },
   "source": [
    "# random starting point\n",
    "localmin = np.random.choice(x,1) #np.array([0])#\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = .01\n",
    "training_epochs = 100\n",
    "\n",
    "# run through training\n",
    "for i in range(training_epochs):\n",
    "  grad = deriv(localmin)\n",
    "  localmin = localmin - learning_rate*grad\n",
    "\n",
    "localmin"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3hIord_pVIU8"
   },
   "source": [
    "# plot the results\n",
    "\n",
    "plt.plot(x,fx(x), x,deriv(x))\n",
    "plt.plot(localmin,deriv(localmin),'ro')\n",
    "plt.plot(localmin,fx(localmin),'ro')\n",
    "\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(['f(x)','df','f(x) min'])\n",
    "plt.title('Empirical local minimum: %s'%localmin[0])\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MAGG5fMMVIhM"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh28k_l29urR"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ib3uQtfv9wE2"
   },
   "source": [
    "# 1) The derivative has a multiplicative factor of 2 in it. Is that constant necessary for the accuracy of the g.d. result?\n",
    "#    Try removing that '2' from the derivative and see whether the model can still find the minimum. Before running the\n",
    "#    code, think about what you expect to happen. Does reality match your expectations? Why is (or isn't) that factor necessary?\n",
    "# \n",
    "# 2) What about the factor of '2' inside the np.sin() function? Is that important? Can you get an accurate result if you\n",
    "#    remove it?\n",
    "# \n",
    "# 3) Try setting the initial value to a small but non-zero number, e.g., .0001 or -.0001. Does that help the solution?\n",
    "# "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}