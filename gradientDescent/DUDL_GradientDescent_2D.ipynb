{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1qFigOOWXcSNyA6hPpZnF-QqhFEB1e-hy",
     "timestamp": 1597210805829
    },
    {
     "file_id": "1kGRo0g3UXxXpJuQSEtpKjEGA1Vxbaz8S",
     "timestamp": 1597128018290
    },
    {
     "file_id": "1U4oG0A3DFC-XBWhvecYeA3YYReqHpShX",
     "timestamp": 1594575042741
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Gradient descent\n",
    "### LECTURE: Gradient descent in 2D\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JL_0UKJOj1YP"
   },
   "source": [
    "# import all necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sym # sympy to compute the partial derivatives\n",
    "\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuu117ieVIj0"
   },
   "source": [
    "# Gradient descent in 2D"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0w52qmG_bj1w"
   },
   "source": [
    "# the \"peaks\" function\n",
    "def peaks(x,y):\n",
    "  # expand to a 2D mesh\n",
    "  x,y = np.meshgrid(x,y)\n",
    "  \n",
    "  z = 3*(1-x)**2 * np.exp(-(x**2) - (y+1)**2) \\\n",
    "      - 10*(x/5 - x**3 - y**5) * np.exp(-x**2-y**2) \\\n",
    "      - 1/3*np.exp(-(x+1)**2 - y**2)\n",
    "  return z"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SFLOlXhVbj5A"
   },
   "source": [
    "# create the landscape\n",
    "x = np.linspace(-3,3,201)\n",
    "y = np.linspace(-3,3,201)\n",
    "\n",
    "Z = peaks(x,y)\n",
    "\n",
    "# let's have a look!\n",
    "plt.imshow(Z,extent=[x[0],x[-1],y[0],y[-1]],vmin=-5,vmax=5,origin='lower')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "guL7sYzwbj_q"
   },
   "source": [
    "# create derivative functions using sympy\n",
    "\n",
    "sx,sy = sym.symbols('sx,sy')\n",
    "\n",
    "sZ = 3*(1-sx)**2 * sym.exp(-(sx**2) - (sy+1)**2) \\\n",
    "      - 10*(sx/5 - sx**3 - sy**5) * sym.exp(-sx**2-sy**2) \\\n",
    "      - 1/3*sym.exp(-(sx+1)**2 - sy**2)\n",
    "\n",
    "\n",
    "# create functions from the sympy-computed derivatives\n",
    "df_x = sym.lambdify( (sx,sy),sym.diff(sZ,sx),'sympy' )\n",
    "df_y = sym.lambdify( (sx,sy),sym.diff(sZ,sy),'sympy' )\n",
    "\n",
    "df_x(1,1).evalf()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mo3H8RpgiUN9"
   },
   "source": [
    "# random starting point (uniform between -2 and +2)\n",
    "localmin = np.random.rand(2)*4-2 # also try specifying coordinates\n",
    "startpnt = localmin[:] # make a copy, not re-assign\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = .01\n",
    "training_epochs = 1000\n",
    "\n",
    "# run through training\n",
    "trajectory = np.zeros((training_epochs,2))\n",
    "for i in range(training_epochs):\n",
    "  grad = np.array([ df_x(localmin[0],localmin[1]).evalf(), \n",
    "                    df_y(localmin[0],localmin[1]).evalf() \n",
    "                  ])\n",
    "  localmin = localmin - learning_rate*grad  # add _ or [:] to change a variable in-place\n",
    "  trajectory[i,:] = localmin\n",
    "\n",
    "\n",
    "print(localmin)\n",
    "print(startpnt)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sRrjCTqFbkCo"
   },
   "source": [
    "# let's have a look!\n",
    "plt.imshow(Z,extent=[x[0],x[-1],y[0],y[-1]],vmin=-5,vmax=5,origin='lower')\n",
    "plt.plot(startpnt[0],startpnt[1],'bs')\n",
    "plt.plot(localmin[0],localmin[1],'ro')\n",
    "plt.plot(trajectory[:,0],trajectory[:,1],'r')\n",
    "plt.legend(['rnd start','local min'])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cY-P0o9vbkJD"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvGE12fEiui7"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m54Y_kYXiwO9"
   },
   "source": [
    "# 1) Modify the code to force the initial guess to be [0,1.4]. Does the model reach a reasonable local minimum?\n",
    "# \n",
    "# 2) Using the same starting point, change the number of training epochs to 10,000. Does the final solution differ from\n",
    "#    using 1000 epochs? \n",
    "# \n",
    "# 3) (Again with the same starting location) Change the learning to .1 (1000 epochs). What do you notice about the trajectory?\n",
    "#    Try again with the learning rate set to .5, and then to .00001.\n",
    "# "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}