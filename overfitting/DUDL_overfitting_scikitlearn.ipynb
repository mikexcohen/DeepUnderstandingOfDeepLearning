{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Overfitting, cross-validation, regularization\n",
    "### LECTURE: Cross-validation -- scikitlearn\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NEW!\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MU7rvmWuhjud"
   },
   "source": [
    "# import dataset\n",
    "import pandas as pd\n",
    "iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "\n",
    "# convert from pandas dataframe to tensor\n",
    "data = torch.tensor( iris[iris.columns[0:4]].values ).float()\n",
    "\n",
    "# transform species to number\n",
    "labels = torch.zeros(len(data), dtype=torch.long)\n",
    "# labels[iris.species=='setosa'] = 0 # don't need!\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica'] = 2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhkvsJ6g6uXr"
   },
   "source": [
    "# A brief aside on using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q-YUb7pW19yy"
   },
   "source": [
    "# create our fake dataset\n",
    "\n",
    "fakedata = np.tile(np.array([1,2,3,4]),(10,1)) + np.tile(10*np.arange(1,11),(4,1)).T\n",
    "fakelabels = np.arange(10)>4\n",
    "print(fakedata), print(' ')\n",
    "print(fakelabels)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8bxbHGkP7JW3"
   },
   "source": [
    "# use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = \\\n",
    "                        train_test_split(fakedata, fakelabels, test_size=.2)\n",
    "\n",
    "# NOTE the third input parameter above.\n",
    "# This can be specified as test size or training size.\n",
    "# Be mindful of which parameter is written!\n",
    "\n",
    "\n",
    "\n",
    "# print out the sizes\n",
    "print('Training data size: ' + str(train_data.shape))\n",
    "print('Test data size: ' + str(test_data.shape))\n",
    "print(' ')\n",
    "\n",
    "# print out the train/test data\n",
    "print('Training data: ')\n",
    "print(train_data)\n",
    "print(' ')\n",
    "\n",
    "print('Test data: ')\n",
    "print(test_data)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuO98C9ESMt2"
   },
   "source": [
    "# Now back to the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v0JMIGb1iV_9"
   },
   "source": [
    "# a function that creates the ANN model\n",
    "\n",
    "def createANewModel():\n",
    "\n",
    "  # model architecture\n",
    "  ANNiris = nn.Sequential(\n",
    "      nn.Linear(4,64),   # input layer\n",
    "      nn.ReLU(),         # activation unit\n",
    "      nn.Linear(64,64),  # hidden layer\n",
    "      nn.ReLU(),         # activation unit\n",
    "      nn.Linear(64,3),   # output units\n",
    "        )\n",
    "\n",
    "  # loss function\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.SGD(ANNiris.parameters(),lr=.01)\n",
    "\n",
    "  return ANNiris,lossfun,optimizer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cVD1nFTli7TO"
   },
   "source": [
    "# train the model\n",
    "\n",
    "# global parameter\n",
    "numepochs = 200\n",
    "\n",
    "def trainTheModel(trainProp):\n",
    "\n",
    "  # initialize losses\n",
    "  losses = torch.zeros(numepochs)\n",
    "  trainAcc = []\n",
    "  testAcc  = []\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # separate train from test data\n",
    "    # Note 1: unique split for each epoch!\n",
    "    # Note 2: here we specify the training size, not the testing size!\n",
    "    X_train,X_test, y_train,y_test = train_test_split(data,labels, train_size=trainProp)\n",
    "\n",
    "\n",
    "    # forward pass and loss\n",
    "    yHat = ANNiris(X_train)\n",
    "    loss = lossfun(yHat,y_train)\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # compute training accuracy\n",
    "    trainAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y_train).float()).item() )\n",
    "\n",
    "    # test accuracy\n",
    "    predlabels = torch.argmax( ANNiris(X_test),axis=1 )\n",
    "    testAcc.append( 100*torch.mean((predlabels == y_test).float()).item() )\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWefd33ISPBd"
   },
   "source": [
    "# Test the model by running it once"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vXku7xIdcu7Y"
   },
   "source": [
    "# create a model\n",
    "ANNiris,lossfun,optimizer = createANewModel()\n",
    "\n",
    "# train the model\n",
    "# NOTE: the input is the training proportion, not the test proportion!\n",
    "trainAcc,testAcc = trainTheModel(.8)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JYouZAY4i3jM"
   },
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(trainAcc,'ro-')\n",
    "plt.plot(testAcc,'bs-')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend(['Train','Test'])\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcbD9nZmd9nu"
   },
   "source": [
    "# Now for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vy0lu1A2BY1h"
   },
   "source": [
    "trainSetSizes = np.linspace(.2,.95,10)\n",
    "\n",
    "allTrainAcc = np.zeros((len(trainSetSizes),numepochs))\n",
    "allTestAcc = np.zeros((len(trainSetSizes),numepochs))\n",
    "\n",
    "for i in range(len(trainSetSizes)):\n",
    "\n",
    "  # create a model\n",
    "  ANNiris,lossfun,optimizer = createANewModel()\n",
    "\n",
    "  # train the model\n",
    "  trainAcc,testAcc = trainTheModel(trainSetSizes[i])\n",
    "\n",
    "  # store the results\n",
    "  allTrainAcc[i,:] = trainAcc\n",
    "  allTestAcc[i,:] = testAcc"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BLkPP3yrCh5b"
   },
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(13,5))\n",
    "\n",
    "ax[0].imshow(allTrainAcc,aspect='auto',\n",
    "             vmin=50,vmax=90, extent=[0,numepochs,trainSetSizes[-1],trainSetSizes[0]])\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Training size proportion')\n",
    "ax[0].set_title('Training accuracy')\n",
    "\n",
    "p = ax[1].imshow(allTestAcc,aspect='auto',\n",
    "             vmin=50,vmax=90, extent=[0,numepochs,trainSetSizes[-1],trainSetSizes[0]])\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Training size proportion')\n",
    "ax[1].set_title('Test accuracy')\n",
    "fig.colorbar(p,ax=ax[1])\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "COCgpMI1JcSc"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwTbABK7fqzZ"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jWC_SDDCfrAo"
   },
   "source": [
    "# 1) The images above suggest that the training proportion doesn't really affect learning success (for this data and this\n",
    "#    model). Does increasing the number of epochs to 1000 change the conclusion? How about with a lr=.001?\n",
    "#\n",
    "# 2) According to the help doc for train_test_split(), the train_size input can be either a float between 0.0 and 1.0, or\n",
    "#    an int. Here we only used float inputs to indicate the proportion of the data used for training. Modify the code to\n",
    "#    specify the training size as an integer corresponding to the number of samples.\n",
    "#"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}