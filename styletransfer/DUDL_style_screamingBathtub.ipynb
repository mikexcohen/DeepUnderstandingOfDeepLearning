{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DUDL_style_screamingBathtub.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyNE9y2IWWpNrq1+kg0hqSmA"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Style transfer\n",
    "### LECTURE: Transfering the screaming bathtub\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j7-LiwqUMGYL"
   },
   "source": [
    "### import libraries\n",
    "\n",
    "# for DL modeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# to read an image from a url\n",
    "from imageio import imread\n",
    "\n",
    "# for number-crunching\n",
    "import numpy as np\n",
    "\n",
    "# for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN5Y7ABfeoNt"
   },
   "source": [
    "# Import VGG19 and freeze all layers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f2FoXgvhesHF"
   },
   "source": [
    "# import the model\n",
    "vggnet = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "# freeze all layers\n",
    "for p in vggnet.parameters():\n",
    "    p.requires_grad = False\n",
    "  \n",
    "# set to evaluation mode\n",
    "vggnet.eval()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EeTXUCtOesdA"
   },
   "source": [
    "# send the network to the GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "vggnet.to(device);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08UaJc5WfV1k"
   },
   "source": [
    "# Import two images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZI2WXfdkfYYX"
   },
   "source": [
    "img4content = imread('https://upload.wikimedia.org/wikipedia/commons/6/61/De_nieuwe_vleugel_van_het_Stedelijk_Museum_Amsterdam.jpg')\n",
    "img4style   = imread('https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg')\n",
    "\n",
    "# initialize the target image and random numbers\n",
    "img4target = np.random.randint(low=0,high=255,size=img4content.shape,dtype=np.uint8)\n",
    "\n",
    "print(img4content.shape)\n",
    "print(img4target.shape)\n",
    "print(img4style.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UnKZOcn8esgP"
   },
   "source": [
    "## These images are really large, which will make training take a long time. \n",
    "\n",
    "# create the transforms\n",
    "Ts = T.Compose([ T.ToTensor(),\n",
    "                 T.Resize(256),\n",
    "                 T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "               ])\n",
    "\n",
    "# apply them to the images (\"unsqueeze\" to make them a 4D tensor) and push to GPU\n",
    "img4content = Ts( img4content ).unsqueeze(0).to(device)\n",
    "img4style   = Ts( img4style   ).unsqueeze(0).to(device)\n",
    "img4target  = Ts( img4target  ).unsqueeze(0).to(device)\n",
    "\n",
    "print(img4content.shape)\n",
    "print(img4target.shape)\n",
    "print(img4style.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nCL4gmObesjX"
   },
   "source": [
    "# Let's have a look at the \"before\" pics\n",
    "fig,ax = plt.subplots(1,3,figsize=(18,6))\n",
    "\n",
    "pic = img4content.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[0].imshow(pic)\n",
    "ax[0].set_title('Content picture')\n",
    "\n",
    "pic = img4target.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[1].imshow(pic)\n",
    "ax[1].set_title('Target picture')\n",
    "\n",
    "pic = img4style.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[2].imshow(pic)\n",
    "ax[2].set_title('Style picture')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQnjY4ojjIbH"
   },
   "source": [
    "# Functions to extract image feature map activations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gX2GQHEIjbSH"
   },
   "source": [
    "# A function that returns feature maps\n",
    "\n",
    "def getFeatureMapActs(img,net):\n",
    "  \n",
    "  # initialize feature maps as a list\n",
    "  featuremaps = []\n",
    "  featurenames = []\n",
    "\n",
    "  convLayerIdx = 0\n",
    "\n",
    "  # loop through all layers in the \"features\" block\n",
    "  for layernum in range(len(net.features)):\n",
    "    \n",
    "    # print out info from this layer\n",
    "    # print(layernum,net.features[layernum])\n",
    "\n",
    "    # process the image through this layer\n",
    "    img = net.features[layernum](img)\n",
    "\n",
    "    # store the image if it's a conv2d layer\n",
    "    if 'Conv2d' in str(net.features[layernum]):\n",
    "      featuremaps.append( img )\n",
    "      featurenames.append( 'ConvLayer_' + str(convLayerIdx) )\n",
    "      convLayerIdx += 1\n",
    "  \n",
    "  return featuremaps,featurenames"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UlcDsv-r5P0H"
   },
   "source": [
    "# A function that returns the Gram matrix of the feature activation map\n",
    "\n",
    "def gram_matrix(M):\n",
    "  \n",
    "  # reshape to 2D\n",
    "  _,chans,height,width = M.shape\n",
    "  M = M.reshape(chans,height*width)  \n",
    "\n",
    "  # compute and return covariance matrix\n",
    "  gram = torch.mm(M,M.t()) / (chans*height*width)\n",
    "  return gram"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6RvCMiW13mmn"
   },
   "source": [
    "# inspect the output of the function\n",
    "featmaps,featnames = getFeatureMapActs(img4content,vggnet)\n",
    "\n",
    "# print out some info\n",
    "for i in range(len(featnames)):\n",
    "  print('Feature map \"%s\" is size %s'%(featnames[i],(featmaps[i].shape)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AbKb21yP2ujR"
   },
   "source": [
    "# let's see what the \"content\" image looks like\n",
    "contentFeatureMaps,contentFeatureNames = getFeatureMapActs(img4content,vggnet)\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(2,5,figsize=(18,6))\n",
    "for i in range(5):\n",
    "\n",
    "  # average over all feature maps from this layer, and normalize\n",
    "  pic = np.mean( contentFeatureMaps[i].cpu().squeeze().numpy() ,axis=0)\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[0,i].imshow(pic,cmap='gray')\n",
    "  axs[0,i].set_title('Content layer ' + str(contentFeatureNames[i]))\n",
    "\n",
    "\n",
    "  ### now show the gram matrix\n",
    "  pic = gram_matrix(contentFeatureMaps[i]).cpu().numpy()\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[1,i].imshow(pic,cmap='gray',vmax=.1)\n",
    "  axs[1,i].set_title('Gram matrix, layer ' + str(contentFeatureNames[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VgLRHR_Cesmf"
   },
   "source": [
    "\n",
    "# repeat for the \"style\" image\n",
    "styleFeatureMaps,styleFeatureNames = getFeatureMapActs(img4style,vggnet)\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(2,5,figsize=(18,6))\n",
    "for i in range(5):\n",
    "\n",
    "  # average over all feature maps from this layer, and normalize\n",
    "  pic = np.mean( styleFeatureMaps[i].cpu().squeeze().numpy() ,axis=0)\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[0,i].imshow(pic,cmap='hot')\n",
    "  axs[0,i].set_title('Style layer ' + str(styleFeatureNames[i]))\n",
    "\n",
    "\n",
    "  ### now show the gram matrix\n",
    "  pic = gram_matrix(styleFeatureMaps[i]).cpu().numpy()\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[1,i].imshow(pic,cmap='hot',vmax=.1)\n",
    "  axs[1,i].set_title('Gram matrix, layer ' + str(styleFeatureNames[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfXfw0Zb5QGe"
   },
   "source": [
    "# Now for the transfer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FHZAzPCOHWQx"
   },
   "source": [
    "# which layers to use\n",
    "layers4content = [ 'ConvLayer_1','ConvLayer_4' ]\n",
    "layers4style   = [ 'ConvLayer_1','ConvLayer_2','ConvLayer_3','ConvLayer_4','ConvLayer_5' ]\n",
    "weights4style  = [      1       ,     .5      ,     .5      ,     .2      ,     .1       ]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8_gMykBE5QJI"
   },
   "source": [
    "# make a copy of the target image and push to GPU\n",
    "target = img4target.clone()\n",
    "target.requires_grad = True\n",
    "target = target.to(device)\n",
    "styleScaling = 1e6\n",
    "\n",
    "# number of epochs to train\n",
    "numepochs = 1500\n",
    "\n",
    "# optimizer for backprop\n",
    "optimizer = torch.optim.RMSprop([target],lr=.005)\n",
    "\n",
    "\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "  # extract the target feature maps\n",
    "  targetFeatureMaps,targetFeatureNames = getFeatureMapActs(target,vggnet)\n",
    "\n",
    "\n",
    "  # initialize the individual loss components\n",
    "  styleLoss = 0\n",
    "  contentLoss = 0\n",
    "\n",
    "  # loop over layers\n",
    "  for layeri in range(len(targetFeatureNames)):\n",
    "\n",
    "\n",
    "    # compute the content loss\n",
    "    if targetFeatureNames[layeri] in layers4content:\n",
    "      contentLoss += torch.mean( (targetFeatureMaps[layeri]-contentFeatureMaps[layeri])**2 )\n",
    "\n",
    "\n",
    "    # compute the style loss\n",
    "    if targetFeatureNames[layeri] in layers4style:\n",
    "      \n",
    "      # Gram matrices\n",
    "      Gtarget = gram_matrix(targetFeatureMaps[layeri])\n",
    "      Gstyle  = gram_matrix(styleFeatureMaps[layeri])\n",
    "\n",
    "      # compute their loss (de-weighted with increasing depth)\n",
    "      styleLoss += torch.mean( (Gtarget-Gstyle)**2 ) * weights4style[layers4style.index(targetFeatureNames[layeri])]\n",
    "\n",
    "  \n",
    "  # combined loss\n",
    "  combiloss = styleScaling*styleLoss + contentLoss\n",
    "\n",
    "  # finally ready for backprop!\n",
    "  optimizer.zero_grad()\n",
    "  combiloss.backward()\n",
    "  optimizer.step()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_o6RsCi5QLz"
   },
   "source": [
    "# Let's have a looksie!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "acf88v0EDYic"
   },
   "source": [
    "# the \"after\" pic\n",
    "fig,ax = plt.subplots(1,3,figsize=(18,11))\n",
    "\n",
    "pic = img4content.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[0].imshow(pic)\n",
    "ax[0].set_title('Content picture',fontweight='bold')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "pic = torch.sigmoid(target).cpu().detach().squeeze().numpy().transpose((1,2,0))\n",
    "ax[1].imshow(pic)\n",
    "ax[1].set_title('Target picture',fontweight='bold')\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "pic = img4style.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[2].imshow(pic,aspect=.6)\n",
    "ax[2].set_title('Style picture',fontweight='bold')\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks([])\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CMqnRgur6k9A"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ_c6cWyIu_x"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CfvdVYtsIvB6"
   },
   "source": [
    "# 1) The minimization loss has two components (style and content). Modify the code to store these two components in a\n",
    "#    Nx2 matrix (for N training epochs). Then plot them. This will help you understand and adjust the styleScaling gain\n",
    "#    factor.\n",
    "# \n",
    "# 2) Change the layers for minimizing losses to content and style images. Do you notice an effect of minimizing the\n",
    "#    earlier vs. later layers? How about more vs. fewer layers?\n",
    "# \n",
    "# 3) It's pretty neat to see the target image evolve over time. Modify the code to save the target image every, e.g.,\n",
    "#    100 epochs. Then you can make a series of images showing how the noise transforms into a lovely picture.\n",
    "# \n",
    "# 4) The target picture was initialized as random noise. But it doesn't need to be. It can be initialized to anything\n",
    "#    else. Try the following target initializations: (1) the content picture; (2) the style picture; (3) a completely\n",
    "#    different picture (e.g., a picture of you or a cat or the Taj Mahal).\n",
    "# "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}