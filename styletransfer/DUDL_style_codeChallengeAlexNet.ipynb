{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2dff7c",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ydebessu/DeepUnderstandingOfDeepLearning/blob/main/styletransfer/DUDL_style_codeChallengeAlexNet.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Style transfer\n",
    "### LECTURE: CodeChallenge: Style transfer with AlexNet\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7-LiwqUMGYL"
   },
   "outputs": [],
   "source": [
    "### import libraries\n",
    "\n",
    "# for DL modeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# to read an image from a url\n",
    "from imageio import imread\n",
    "\n",
    "# for number-crunching\n",
    "import numpy as np\n",
    "\n",
    "# for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN5Y7ABfeoNt"
   },
   "source": [
    "# Import AlexNet and freeze all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2FoXgvhesHF"
   },
   "outputs": [],
   "source": [
    "# import the model\n",
    "alexnet = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "# freeze all layers\n",
    "for p in alexnet.parameters():\n",
    "    p.requires_grad = False\n",
    "  \n",
    "# set to evaluation mode\n",
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeTXUCtOesdA"
   },
   "outputs": [],
   "source": [
    "# send the network to the GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "alexnet.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08UaJc5WfV1k"
   },
   "source": [
    "# Import two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI2WXfdkfYYX"
   },
   "outputs": [],
   "source": [
    "img4content = imread('https://images-na.ssl-images-amazon.com/images/I/61A6DiLZeWL.jpg')\n",
    "img4style   = imread('https://upload.wikimedia.org/wikipedia/commons/d/d4/Abstract_stained_glass_window_in_cathedral_of_St._Charles_Borromeo_in_Ciudad_Quesada.jpg')\n",
    "\n",
    "# initialize the target image and random numbers\n",
    "img4target = np.random.randint(low=0,high=255,size=img4content.shape,dtype=np.uint8)\n",
    "\n",
    "print(img4content.shape)\n",
    "print(img4target.shape)\n",
    "print(img4style.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnKZOcn8esgP"
   },
   "outputs": [],
   "source": [
    "## These images are really large, which will make training take a long time. \n",
    "\n",
    "# create the transforms\n",
    "Ts = T.Compose([ T.ToTensor(),\n",
    "                 T.Resize(256),\n",
    "                 T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "               ])\n",
    "\n",
    "# apply them to the images (\"unsqueeze\" to make them a 4D tensor) and push to GPU\n",
    "img4content = Ts( img4content ).unsqueeze(0).to(device)\n",
    "img4style   = Ts( img4style ).unsqueeze(0).to(device)\n",
    "img4target  = Ts( img4target ).unsqueeze(0).to(device)\n",
    "\n",
    "print(img4content.shape)\n",
    "print(img4target.shape)\n",
    "print(img4style.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCL4gmObesjX"
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the \"before\" pics\n",
    "fig,ax = plt.subplots(1,3,figsize=(18,6))\n",
    "\n",
    "pic = img4content.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[0].imshow(pic)\n",
    "ax[0].set_title('Content picture')\n",
    "\n",
    "pic = img4target.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[1].imshow(pic)\n",
    "ax[1].set_title('Target picture')\n",
    "\n",
    "pic = img4style.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[2].imshow(pic)\n",
    "ax[2].set_title('Style picture')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQnjY4ojjIbH"
   },
   "source": [
    "# Functions to extract image feature map activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX2GQHEIjbSH"
   },
   "outputs": [],
   "source": [
    "# A function that returns feature maps\n",
    "\n",
    "def getFeatureMapActs(img,net):\n",
    "  \n",
    "  # initialize feature maps as a list\n",
    "  featuremaps = []\n",
    "  featurenames = []\n",
    "\n",
    "  convLayerIdx = 0\n",
    "\n",
    "  # loop through all layers in the \"features\" block\n",
    "  for layernum in range(len(net.features)):\n",
    "    \n",
    "    # print out info from this layer\n",
    "    # print(layernum,net.features[layernum])\n",
    "\n",
    "    # process the image through this layer\n",
    "    img = net.features[layernum](img)\n",
    "\n",
    "    # store the image if it's a conv2d layer\n",
    "    if 'Conv2d' in str(net.features[layernum]):\n",
    "      featuremaps.append( img )\n",
    "      featurenames.append( 'ConvLayer_' + str(convLayerIdx) )\n",
    "      convLayerIdx += 1\n",
    "  \n",
    "  return featuremaps,featurenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlcDsv-r5P0H"
   },
   "outputs": [],
   "source": [
    "# A function that returns the Gram matrix of the feature activation map\n",
    "\n",
    "def gram_matrix(F):\n",
    "  \n",
    "  # reshape to 2D\n",
    "  _,chans,height,width = F.shape\n",
    "  F = F.reshape(chans,height*width)  \n",
    "\n",
    "  # compute and return covariance matrix\n",
    "  gram = torch.mm(F,F.t()) / (chans*height*width)\n",
    "  return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RvCMiW13mmn"
   },
   "outputs": [],
   "source": [
    "# inspect the output of the function\n",
    "featmaps,featnames = getFeatureMapActs(img4content,alexnet)\n",
    "\n",
    "# print out some info\n",
    "for i in range(len(featnames)):\n",
    "  print('Feature map \"%s\" is size %s'%(featnames[i],(featmaps[i].shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbKb21yP2ujR"
   },
   "outputs": [],
   "source": [
    "\n",
    "# let's see what the \"content\" image looks like\n",
    "contentFeatureMaps,contentFeatureNames = getFeatureMapActs(img4content,alexnet)\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(2,5,figsize=(18,6))\n",
    "for i in range(5):\n",
    "\n",
    "  # average over all feature maps from this layer, and normalize\n",
    "  pic = np.mean( contentFeatureMaps[i].cpu().squeeze().numpy() ,axis=0)\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[0,i].imshow(pic,cmap='gray')\n",
    "  axs[0,i].set_title('Content layer ' + str(contentFeatureNames[i]))\n",
    "\n",
    "\n",
    "  ### now show the gram matrix\n",
    "  pic = gram_matrix(contentFeatureMaps[i]).cpu().numpy()\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[1,i].imshow(pic,cmap='gray',vmax=.1)\n",
    "  axs[1,i].set_title('Gram matrix, layer ' + str(contentFeatureNames[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgLRHR_Cesmf"
   },
   "outputs": [],
   "source": [
    "\n",
    "# repeat for the \"style\" image\n",
    "styleFeatureMaps,styleFeatureNames = getFeatureMapActs(img4style,alexnet)\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(2,5,figsize=(18,6))\n",
    "for i in range(5):\n",
    "\n",
    "  # average over all feature maps from this layer, and normalize\n",
    "  pic = np.mean( styleFeatureMaps[i].cpu().squeeze().numpy() ,axis=0)\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[0,i].imshow(pic,cmap='Set1')\n",
    "  axs[0,i].set_title('Style layer ' + str(styleFeatureNames[i]))\n",
    "\n",
    "\n",
    "  ### now show the gram matrix\n",
    "  pic = gram_matrix(styleFeatureMaps[i]).cpu().numpy()\n",
    "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "\n",
    "  axs[1,i].imshow(pic,cmap='jet',vmax=.1)\n",
    "  axs[1,i].set_title('Gram matrix, layer ' + str(styleFeatureNames[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfXfw0Zb5QGe"
   },
   "source": [
    "# Now for the transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHZAzPCOHWQx"
   },
   "outputs": [],
   "source": [
    "# which layers to use\n",
    "layers4content = [ 'ConvLayer_0']#,'ConvLayer_1','ConvLayer_2','ConvLayer_3' ]\n",
    "layers4style   = [ 'ConvLayer_0','ConvLayer_1','ConvLayer_2','ConvLayer_3','ConvLayer_4' ]\n",
    "weights4style  = [      1       ,      .8      ,     .6      ,    .4      ,      .2      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_gMykBE5QJI"
   },
   "outputs": [],
   "source": [
    "# make a copy of the target image and push to GPU\n",
    "target = img4content.clone()\n",
    "target.requires_grad = True\n",
    "target = target.to(device)\n",
    "styleScaling = 5e4\n",
    "\n",
    "# number of epochs to train\n",
    "numepochs = 1500\n",
    "\n",
    "# optimizer for backprop\n",
    "optimizer = torch.optim.RMSprop([target],lr=.001)\n",
    "\n",
    "\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "  # extract the target feature maps\n",
    "  targetFeatureMaps,targetFeatureNames = getFeatureMapActs(target,alexnet)\n",
    "\n",
    "\n",
    "  # initialize the individual loss components\n",
    "  styleLoss = 0\n",
    "  contentLoss = 0\n",
    "\n",
    "  # loop over layers\n",
    "  for layeri in range(len(targetFeatureNames)):\n",
    "\n",
    "\n",
    "    # compute the content loss\n",
    "    if targetFeatureNames[layeri] in layers4content:\n",
    "      contentLoss += torch.mean( (targetFeatureMaps[layeri]-contentFeatureMaps[layeri])**2 )\n",
    "\n",
    "\n",
    "    # compute the style loss\n",
    "    if targetFeatureNames[layeri] in layers4style:\n",
    "      \n",
    "      # Gram matrices\n",
    "      Gtarget = gram_matrix(targetFeatureMaps[layeri])\n",
    "      Gstyle  = gram_matrix(styleFeatureMaps[layeri])\n",
    "\n",
    "      # compute their loss (de-weighted with increasing depth)\n",
    "      styleLoss += torch.mean( (Gtarget-Gstyle)**2 ) * weights4style[layers4style.index(targetFeatureNames[layeri])]\n",
    "\n",
    "  \n",
    "  # combined loss\n",
    "  combiloss = styleScaling*styleLoss + contentLoss\n",
    "\n",
    "  # finally ready for backprop!\n",
    "  optimizer.zero_grad()\n",
    "  combiloss.backward()\n",
    "  optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_o6RsCi5QLz"
   },
   "source": [
    "# Let's have a looksie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acf88v0EDYic"
   },
   "outputs": [],
   "source": [
    "# the \"after\" pic\n",
    "fig,ax = plt.subplots(1,3,figsize=(18,11))\n",
    "\n",
    "pic = img4content.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[0].imshow(pic)\n",
    "ax[0].set_title('Content picture',fontweight='bold')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "pic = torch.sigmoid(target).cpu().detach().squeeze().numpy().transpose((1,2,0))\n",
    "ax[1].imshow(pic)\n",
    "ax[1].set_title('Target picture',fontweight='bold')\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "pic = img4style.cpu().squeeze().numpy().transpose((1,2,0))\n",
    "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "ax[2].imshow(pic,aspect=.6)\n",
    "ax[2].set_title('Style picture',fontweight='bold')\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMqnRgur6k9A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNrT4GHpqxtQLKf/X+40Pul",
   "collapsed_sections": [],
   "name": "DUDL_style_codeChallengeAlexNet.ipynb",
   "provenance": [
    {
     "file_id": "1j01OZeVqaSNNFuXNQYlWPSCIjn_6v92t",
     "timestamp": 1620651858706
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
