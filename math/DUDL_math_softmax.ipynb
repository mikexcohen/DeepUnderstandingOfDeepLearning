{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DUDL_math_softmax.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP3eGKdOz6wpqWd2dmYzvBT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bhWV8oes-wKR"},"source":["# COURSE: A deep understanding of deep learning\n","## SECTION: Math prerequisites\n","### LECTURE: Softmax\n","#### TEACHER: Mike X Cohen, sincxpress.com\n","##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202302"]},{"cell_type":"code","metadata":{"id":"2TD8IyfBGXiY"},"source":["# import libraries\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmjUxlEqGbDu"},"source":["# \"manually\" in numpy\n","\n","# the list of numbers\n","z = [1,2,3]\n","\n","# compute the softmax result\n","num = np.exp(z)\n","den = np.sum( np.exp(z) )\n","sigma = num / den\n","\n","print(sigma)\n","print(np.sum(sigma))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sOug_tPzHY1y"},"source":["# repeat with some random integers\n","z = np.random.randint(-5,high=15,size=25)\n","print(z)\n","\n","# compute the softmax result\n","num = np.exp(z)\n","den = np.sum( num )\n","sigma = num / den\n","\n","# compare\n","plt.plot(z,sigma,'ko')\n","plt.xlabel('Original number (z)')\n","plt.ylabel('Softmaxified $\\sigma$')\n","plt.yscale('log')\n","plt.title('$\\sum\\sigma$ = %g' %np.sum(sigma))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jDIYW2bKHxXB"},"source":["# Using pytorch"]},{"cell_type":"code","metadata":{"id":"TxerJIwvIREg"},"source":["# slightly more involved using torch.nn\n","\n","# create an instance of the softmax activation class\n","softfun = nn.Softmax(dim=0)\n","\n","# then apply the data to that function\n","sigmaT = softfun( torch.Tensor(z) )\n","\n","# now we get the results\n","print(sigmaT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WY4hMa2BIRLQ"},"source":["# show that they are the same\n","plt.plot(sigma,sigmaT,'ko')\n","plt.xlabel('\"Manual\" softmax')\n","plt.ylabel('Pytorch nn.Softmax')\n","plt.title(f'The two methods correlate at r={np.corrcoef(sigma,sigmaT)[0,1]}')\n","plt.show()"],"execution_count":null,"outputs":[]}]}