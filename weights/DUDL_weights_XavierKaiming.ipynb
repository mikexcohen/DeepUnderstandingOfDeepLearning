{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Zh1wPFY2Qqr3Mn9Qr3DyhMinJTjIk1Hz",
     "timestamp": 1619014057075
    },
    {
     "file_id": "1zk0u7ZpzZ38GbX9ExPJRi-V2gSHX3DZX",
     "timestamp": 1618934353597
    },
    {
     "file_id": "13SFr82QoaJr9so_o_rSl1L9WbOMXpUSR",
     "timestamp": 1618861176115
    },
    {
     "file_id": "1FcEBC0NAESIlHQkv6_85R-XDUKGE8XbM",
     "timestamp": 1618848117844
    },
    {
     "file_id": "1qKgZ8kVcqNgwtBzHbWq5yJH_HqI6DxWW",
     "timestamp": 1617803880910
    },
    {
     "file_id": "15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4",
     "timestamp": 1617737766196
    },
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617734878578
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ],
   "authorship_tag": "ABX9TyMs1P8mM0dPFRkp4ayjvr7B"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Weight inits and investigations\n",
    "### LECTURE: Xavier and Kaiming initializations\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK8Opkhgp0bO"
   },
   "source": [
    "# Create the DL model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JK3OO3tAtZkA"
   },
   "source": [
    "# create a class for the model\n",
    "class thenet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    ### input layer\n",
    "    self.input = nn.Linear(100,100)\n",
    "    \n",
    "    ### hidden layer\n",
    "    self.fc1 = nn.Linear(100,100)\n",
    "    self.fc2 = nn.Linear(100,100)\n",
    "    self.fc3 = nn.Linear(100,100)\n",
    "\n",
    "    ### output layer\n",
    "    self.output = nn.Linear(100,2)\n",
    "\n",
    "  # forward pass\n",
    "  def forward(self,x):\n",
    "    x = F.relu( self.input(x) )\n",
    "    x = F.relu( self.fc1(x) )\n",
    "    x = F.relu( self.fc2(x) )\n",
    "    x = F.relu( self.fc3(x) )\n",
    "    return self.output(x)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTgvV8W-Ayem"
   },
   "source": [
    "# Explore the initialized weights"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ou9OauyNA0y9"
   },
   "source": [
    "# create an instance of the model\n",
    "net = thenet()\n",
    "print(net)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xTj9VThexyBd"
   },
   "source": [
    "# collect all weights and biases\n",
    "allweight = np.array([])\n",
    "allbiases = np.array([])\n",
    "\n",
    "for p in net.named_parameters():\n",
    "  if 'bias' in p[0]:\n",
    "    allbiases = np.concatenate( (allbiases,p[1].data.numpy().flatten()),axis=0 )\n",
    "  elif 'weight' in p[0]:\n",
    "    allweight = np.concatenate( (allweight,p[1].data.numpy().flatten()),axis=0 )\n",
    "\n",
    "\n",
    "# how many are there?\n",
    "print(f'There are {len(allbiases)} bias parameters.')\n",
    "print(f'There are {len(allweight)} weight parameters.')\n",
    "\n",
    "\n",
    "# show their histograms\n",
    "fig,ax = plt.subplots(1,3,figsize=(18,4))\n",
    "\n",
    "ax[0].hist(allbiases,40)\n",
    "ax[0].set_title('Histogram of initial biases')\n",
    "\n",
    "\n",
    "ax[1].hist(allweight,40)\n",
    "ax[1].set_title('Histogram of initial weights')\n",
    "\n",
    "\n",
    "\n",
    "# collect histogram data to show as line plots\n",
    "yB,xB = np.histogram(allbiases,30)\n",
    "yW,xW = np.histogram(allweight,30)\n",
    "\n",
    "ax[2].plot((xB[1:]+xB[:-1])/2,yB/np.sum(yB),label='Bias')\n",
    "ax[2].plot((xW[1:]+xW[:-1])/2,yW/np.sum(yW),label='Weight')\n",
    "ax[2].set_title('Density estimate for both')\n",
    "ax[2].legend()\n",
    "\n",
    "\n",
    "# plot adjustments common to all subplots\n",
    "for i in range(3):\n",
    "  ax[i].set_xlabel('Initial value')\n",
    "  ax[i].set_ylabel('Count')\n",
    "ax[2].set_ylabel('Probability')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2rpKarsEegk"
   },
   "source": [
    "# Layer-specific distributions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gHzKOZjnp0qn"
   },
   "source": [
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,4))\n",
    "\n",
    "for p in net.named_parameters():\n",
    "\n",
    "  # get the data and compute their histogram\n",
    "  thesedata = p[1].data.numpy().flatten()\n",
    "  y,x = np.histogram(thesedata,10)\n",
    "\n",
    "  # for the bias\n",
    "  if 'bias' in p[0]:\n",
    "    ax[0].plot((x[1:]+x[:-1])/2,y/np.sum(y),label='%s bias (N=%g)'%(p[0][:-5],len(thesedata)))\n",
    "\n",
    "  # for the weights\n",
    "  elif 'weight' in p[0]:\n",
    "    ax[1].plot((x[1:]+x[:-1])/2,y/np.sum(y),label='%s weight (N=%g)'%(p[0][:-7],len(thesedata)))\n",
    "\n",
    "\n",
    "\n",
    "ax[0].set_title('Biases per layer')\n",
    "ax[0].legend()\n",
    "ax[1].set_title('Weights per layer')\n",
    "ax[1].legend(bbox_to_anchor=(1,1),loc='upper left')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S816nTJqEoic"
   },
   "source": [
    "# What's up with the weird output bias distribution??\n",
    "print( net.output.bias.data )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VOsqp9Dosb1i"
   },
   "source": [
    "# Check out the docstring for linear layers\n",
    "nn.Linear?"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EBe08E8B8SML"
   },
   "source": [
    "# Let's test whether the numbers match our prediction from the formula\n",
    "\n",
    "# empirical bias range\n",
    "biasrange = [ torch.min(net.fc1.bias.data).item(), torch.max(net.fc1.bias.data).item() ]\n",
    "biascount = len(net.fc1.bias.data)\n",
    "\n",
    "# theoretical expected value\n",
    "sigma = np.sqrt(1/biascount)\n",
    "\n",
    "# drum rolllllll.....\n",
    "print('Theoretical sigma = ' + str(sigma))\n",
    "print('Empirical range = ' + str(biasrange))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bxBj7jNr8SSR"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF8oj1EuFLZs"
   },
   "source": [
    "# Now to initialize the weights using the Xavier method"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f_W7VDSGFLZ5"
   },
   "source": [
    "# create a new instance of the model\n",
    "net = thenet()\n",
    "\n",
    "# change the weights (leave biases as Kaiming [default])\n",
    "for p in net.named_parameters():\n",
    "  if 'weight' in p[0]:\n",
    "    nn.init.xavier_normal_(p[1].data)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hCctIrf48RyC"
   },
   "source": [
    "# Scroll up and re-run the previous weights visualization cells with the new network.\n",
    "# Then continue below."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mXiKnae_JfWV"
   },
   "source": [
    "# Let's test whether the numbers match our prediction from the formula\n",
    "\n",
    "# empirical weight standard deviation\n",
    "weightvar   = torch.var(net.fc1.weight.data.flatten()).item()\n",
    "weightcount = len(net.fc1.weight.data)\n",
    "\n",
    "# theoretical expected value\n",
    "sigma2 = 2 / (weightcount+weightcount)\n",
    "\n",
    "# drum rolllllll.....\n",
    "print('Theoretical sigma = ' + str(sigma2))\n",
    "print('Empirical variance = ' + str(weightvar))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pjgqWxRUJfyQ"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OqR9br44HYGC"
   },
   "source": [
    "# Note: There are several other weights initialization methods availabe in PyTorch.\n",
    "#       See https://pytorch.org/docs/stable/nn.init.html"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KP-981UsbjS"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JdnIdmBjsd2U"
   },
   "source": [
    "# 1) Explore the weight initialization options using PyTorch's functions (nn.init.<method>). \n",
    "#    For example: apply Xavier-uniform, Kaiming, constant (this is what we did in the first video of this section).\n",
    "# "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}