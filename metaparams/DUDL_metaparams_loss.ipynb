{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DUDL_metaparams_loss.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyM8nF/yQRb+pqekb1hGILaP"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Metaparameters (activation, batch, optimizers)\n",
    "### LECTURE: Loss functions in Pytorch\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oMl3PkezSPHA"
   },
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_fNhEIyU8-5"
   },
   "source": [
    "# Mean-squared error"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kww9GdqRS1_p"
   },
   "source": [
    "# loss function\n",
    "lossfunMSE = nn.MSELoss()\n",
    "\n",
    "# create predictions and real answer\n",
    "yHat = torch.linspace(-2,2,101)\n",
    "y = torch.tensor(.5)\n",
    "\n",
    "# compute MSE loss function\n",
    "L = np.zeros(101)\n",
    "for i,yy in enumerate(yHat):\n",
    "  L[i] = lossfunMSE(yy,y)\n",
    "\n",
    "plt.plot(yHat,L,label='Loss')\n",
    "plt.plot([y,y],[0,np.max(L)],'r--',label='True value')\n",
    "plt.xlabel('Predicted value')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW_RBi6wU65j"
   },
   "source": [
    "# Binary cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HiN5ni12VEq0"
   },
   "source": [
    "# loss function\n",
    "lossfunBCE = nn.BCELoss()\n",
    "\n",
    "# create predictions and real answer\n",
    "yHat = torch.linspace(.001,.999,101)\n",
    "y1 = torch.tensor(0.)\n",
    "y2 = torch.tensor(1.)\n",
    "\n",
    "# compute MSE loss function\n",
    "L = np.zeros((101,2))\n",
    "for i,yy in enumerate(yHat):\n",
    "  L[i,0] = lossfunBCE(yy,y1) # 0 is the correct answer\n",
    "  L[i,1] = lossfunBCE(yy,y2) # 1 is the correct answer\n",
    "\n",
    "plt.plot(yHat,L)\n",
    "plt.xlabel('Predicted value')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['correct=0','correct=1'])\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J_SJFL9CBVGA"
   },
   "source": [
    "# The example above shows data already in probabilities. Raw outputs will need to be converted to probabilities:\n",
    "\n",
    "# \"raw\" output of a model\n",
    "yHat = torch.tensor(2.)\n",
    "print(lossfunBCE(yHat,y2))\n",
    "\n",
    "# convert to prob via sigmoid\n",
    "sig = nn.Sigmoid()\n",
    "print(lossfunBCE( sig(yHat) ,y2))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wQmVOg-iCLGB"
   },
   "source": [
    "# However, PyTorch recommends using a single function that incorporates sigmoid+BCE due to increased numerical stability.\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html?highlight=nn%20bcewithlogitsloss#torch.nn.BCEWithLogitsLoss\n",
    "\n",
    "\n",
    "# Thus, the recommended way to do it:\n",
    "lossfunBCE = nn.BCEWithLogitsLoss()\n",
    "yHat = torch.tensor(2.)\n",
    "print(lossfunBCE(yHat,y2))\n",
    "\n",
    "# In toy examples, numerical accuracy usually isn't a problem."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TU_1uyOHWG7w"
   },
   "source": [
    "# Categorical cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A868kRI3VEts"
   },
   "source": [
    "# loss function\n",
    "lossfunCCE = nn.CrossEntropyLoss()\n",
    "\n",
    "# vector of output layer (pre-softmax)\n",
    "yHat = torch.tensor([[1.,4,3]])\n",
    "\n",
    "for i in range(3):\n",
    "  correctAnswer = torch.tensor([i])\n",
    "  thisloss = lossfunCCE(yHat,correctAnswer).item()\n",
    "  print( 'Loss when correct answer is %g: %g' %(i,thisloss) )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f5sW84OScqNu"
   },
   "source": [
    "# Repeat using pre-softmaxified output\n",
    "sm = nn.Softmax(dim=1)\n",
    "yHat_sm = sm(yHat)\n",
    "\n",
    "for i in range(3):\n",
    "  correctAnswer = torch.tensor([i])\n",
    "  thisloss = lossfunCCE(yHat_sm,correctAnswer).item()\n",
    "  print( 'Loss when correct answer is %g: %g' %(i,thisloss) )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PskVgu-gK66H"
   },
   "source": [
    "# compare raw, softmax, and log-softmax outputs\n",
    "sm = nn.LogSoftmax(dim=1)\n",
    "yHat_logsm = sm(yHat)\n",
    "\n",
    "# print them\n",
    "print(yHat)\n",
    "print(yHat_sm)\n",
    "print(yHat_logsm)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGtELAAfErbk"
   },
   "source": [
    "# Creating your own custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DlCcAbitHhpK"
   },
   "source": [
    "class myLoss(nn.Module): # inherent info from nn.Module\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "      \n",
    "  def forward(self,x,y):\n",
    "    loss = torch.abs(x-y)\n",
    "    return loss\n",
    "\n",
    "# test it out!\n",
    "lfun = myLoss()\n",
    "lfun(torch.tensor(4),torch.tensor(5.2))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}