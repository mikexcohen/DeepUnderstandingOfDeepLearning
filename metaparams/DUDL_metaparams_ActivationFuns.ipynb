{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DUDL_metaparams_ActivationFuns.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyMUqAhy1F4s3zsVEaNYgaca"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Metaparameters (activation, batch, optimizers)\n",
    "### LECTURE: Activation functions\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7U3TmybM4yMw"
   },
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':18})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lFT18TkBwwuH"
   },
   "source": [
    "# variable to evaluate over\n",
    "x = torch.linspace(-3,3,101)\n",
    "\n",
    "# create a function that returns the activated output\n",
    "def NNoutputx(actfun):\n",
    "  # get activation function type\n",
    "  # this code replaces torch.relu with torch.<actfun>\n",
    "  actfun = getattr(torch,actfun)\n",
    "  return actfun( x )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PZmnZmeBxn8-"
   },
   "source": [
    "# the activation functions\n",
    "activation_funs = [ 'relu', 'sigmoid', 'tanh' ]\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "for actfun in activation_funs:\n",
    "  plt.plot(x,NNoutputx(actfun),label=actfun,linewidth=3)\n",
    "\n",
    "# add reference lines\n",
    "dashlinecol = [.7,.7,.7]\n",
    "plt.plot(x[[0,-1]],[0,0],'--',color=dashlinecol)\n",
    "plt.plot(x[[0,-1]],[1,1],'--',color=dashlinecol)\n",
    "plt.plot([0,0],[-1,3],'--',color=dashlinecol)\n",
    "\n",
    "# make the plot look nicer\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\sigma(x)$')\n",
    "plt.title('Various activation functions')\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.ylim([-1,3])\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7VOdoMONKT7"
   },
   "source": [
    "# More activation functions in torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pnVfqv9nMck5"
   },
   "source": [
    "# create a function that returns the activated output FUNCTION\n",
    "# this is different from the previous function\n",
    "def NNoutput(actfun):\n",
    "  # get activation function type\n",
    "  # this code replaces torch.nn.relu with torch.nn.<actfun>\n",
    "  actfun = getattr(torch.nn,actfun)\n",
    "  return actfun()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4ItPVGbkNWfb"
   },
   "source": [
    "# the activation functions\n",
    "activation_funs = [ 'ReLU6', 'Hardshrink', 'LeakyReLU' ]\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "for actfun in activation_funs:\n",
    "  plt.plot(x,NNoutput(actfun)(x),label=actfun,linewidth=3)\n",
    "\n",
    "# add reference lines\n",
    "dashlinecol = [.7,.7,.7]\n",
    "plt.plot(x[[0,-1]],[0,0],'--',color=dashlinecol)\n",
    "plt.plot(x[[0,-1]],[1,1],'--',color=dashlinecol)\n",
    "plt.plot([0,0],[-1,3],'--',color=dashlinecol)\n",
    "\n",
    "# make the plot look nicer\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\sigma(x)$')\n",
    "plt.title('Various activation functions')\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.ylim([-1,3])\n",
    "# plt.ylim([-.1,.1])\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bBtcDNRjOcj4"
   },
   "source": [
    "# relu6 in more detail\n",
    "x = torch.linspace(-3,9,101)\n",
    "relu6 = torch.nn.ReLU6()\n",
    "\n",
    "plt.plot(x,relu6(x))\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1hnP2hHPE7A"
   },
   "source": [
    "# Differences between torch and torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C33H5AjfPHuK"
   },
   "source": [
    "# redefine x (fewer points to facilitate visualization)\n",
    "x = torch.linspace(-3,3,21)\n",
    "\n",
    "# in torch\n",
    "y1 = torch.relu(x)\n",
    "\n",
    "# in torch.nn\n",
    "f = torch.nn.ReLU()\n",
    "y2 = f(x)\n",
    "\n",
    "\n",
    "# the results are the same\n",
    "plt.plot(x,y1,'ro',label='torch.relu')\n",
    "plt.plot(x,y2,'bx',label='torch.nn.ReLU')\n",
    "plt.legend()\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YqoLHyHWPH_C"
   },
   "source": [
    "# List of activation functions in PyTorch:\n",
    "#  https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S-LaVpBBGP4W"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwReGp2XHgbX"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sKgqZw-IHiWN"
   },
   "source": [
    "# The goal of these explorations is to help you appreciate the remarkably diverse nonlinear shapes that a node can produce.\n",
    "# All explorations use the code below."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yfn5FakGGPte"
   },
   "source": [
    "# create input vectors\n",
    "x1 = torch.linspace(-1,1,20)\n",
    "x2 = 2*x1\n",
    "\n",
    "# and corresponding weights\n",
    "w1 = -.3\n",
    "w2 = .5\n",
    "\n",
    "# their linear combination\n",
    "linpart = x1*w1 + x2*w2\n",
    "\n",
    "# and the nonlinear output\n",
    "y = torch.relu(linpart)\n",
    "\n",
    "# and plot!\n",
    "plt.plot(x1,linpart,'bo-',label='Linear input')\n",
    "plt.plot(x1,y,'rs',label='Nonlinear output')\n",
    "plt.ylabel('$\\\\hat{y}$ (output of activation function)')\n",
    "plt.xlabel('x1 variable')\n",
    "# plt.ylim([-.1,.1]) # optional -- uncomment and modify to zoom in\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i-5O-ccMGPf_"
   },
   "source": [
    "# 1) Look through the code to make sure you understand what it does (linear weighted combination -> nonlinear function).\n",
    "# \n",
    "# 2) Set x2=x1**2 and run the code. Then set one of the weights to be negative. Then set the negative weight to be close\n",
    "#    to zero (e.g., -.01) with the positive weight relatively large (e.g., .8). Then swap the signs.\n",
    "# \n",
    "# 3) Set x2=x1**2, and set the weights to be .4 and .6. Now set w2=.6 (you might want to zoom in on the y-axis).\n",
    "# \n",
    "# 4) Set x2 to be the absolute value of x1 and both weights positive. Then set w2=-.6. Why does w2<0 have such a big impact?\n",
    "#    More generally, under what conditions are the input and output identical? \n",
    "# \n",
    "# 5) Have fun! Spend a few minutes playing around with the code. Also try changing the activation function to tanh or \n",
    "#    anything else. The goal is to see that really simple input functions with really simple weights can produce really\n",
    "#    complicated-looking nonlinear outputs.\n",
    "# "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}